{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528e0c46-71b2-4cea-91d3-7f6437a5dda8",
   "metadata": {},
   "source": [
    "# ***2023 IFLYTEK MachineLearning-Demo***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc9cb7-b9f1-4b82-9395-1cc1f61fa36c",
   "metadata": {},
   "source": [
    "## ***Section I: Data-Preprocess***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b386bec2-4fe9-4239-a57e-de7d0e66edc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/train/lh.MeanCurv - 1600.csv')\n",
    "df = df.iloc[:, 1:]\n",
    "print(type(df.rolling(3).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d3e216-3e05-442a-82c5-b90ea91df027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from RVM import *\n",
    "\n",
    "def dataaug_rollingavg(ori_df, rolling_avg_size):\n",
    "    dataaug_df = (ori_df.rolling(rolling_avg_size).sum())\n",
    "    return dataaug_df\n",
    "\n",
    "def train_data_preprocess():\n",
    "    data_path_1 = r'data/train/lh.MeanCurv - 1600.csv'\n",
    "    data_path_2 = r'data/train/lh.GausCurv - 1600.csv'\n",
    "    data_path_3 = r'data/train/rh.MeanCurv- 1600.csv'\n",
    "    data_path_4 = r'data/train/rh.GausCurv- 1600.csv'\n",
    "\n",
    "    data_path_5 = r'data/train/lh.ThickAvg - 1600.csv'\n",
    "    data_path_6 = r'data/train/lh.SurfArea - 1600.csv'\n",
    "    data_path_7 = r'data/train/rh.ThickAvg- 1600.csv'\n",
    "    data_path_8 = r'data/train/rh.SurfArea - 1600.csv'\n",
    "    \n",
    "    data_path_9 = r'data/train/lh.GrayVol - 1600.csv'\n",
    "    data_path_10 = r'data/train/rh.GrayVol- 1600.csv'\n",
    "    \n",
    "    data_path_11 = r'data/train/wmparc - 1600.csv'\n",
    "    data_path_12 = r'data/train/aseg - 1600.csv'\n",
    "\n",
    "    label_path = r'data/train/subject_info - 1600.csv'\n",
    "\n",
    "    original_x1, original_x2, original_x3, original_x4, original_y = pd.read_csv(data_path_1), pd.read_csv(\n",
    "        data_path_2), pd.read_csv(data_path_3), pd.read_csv(data_path_4), pd.read_csv(label_path)\n",
    "    original_x5, original_x6, original_x7, original_x8 = pd.read_csv(data_path_5), pd.read_csv(\n",
    "        data_path_6), pd.read_csv(data_path_7), pd.read_csv(data_path_8)\n",
    "    original_x9, original_x10 = pd.read_csv(data_path_9), pd.read_csv(data_path_10)\n",
    "    original_x11, original_x12 = pd.read_csv(data_path_11), pd.read_csv(data_path_12)\n",
    "    \n",
    "    original_x1, original_x2, original_x3, original_x4 = original_x1.iloc[:, 1:], original_x2.iloc[:, 1:], original_x3.iloc[:, 1:], original_x4.iloc[:, 1:]\n",
    "    original_x5, original_x6, original_x7, original_x8 = original_x5.iloc[:, 1:], original_x6.iloc[:, 1:], original_x7.iloc[:, 1:], original_x8.iloc[:, 1:]\n",
    "    original_x9, original_x10 = original_x9.iloc[:, 1:], original_x10.iloc[:, 1:]\n",
    "    original_x11, original_x12 = original_x11.iloc[:, 1:], original_x12.iloc[:, 1:]\n",
    "    original_y = original_y.iloc[:, 3]\n",
    "    \n",
    "    # 目前经验是需要标准化    \n",
    "    original_x1 = (original_x1 - original_x1.min()) / (original_x1.max() - original_x1.min())\n",
    "    original_x2 = (original_x2 - original_x2.min()) / (original_x2.max() - original_x2.min())\n",
    "    original_x3 = (original_x3 - original_x3.min()) / (original_x3.max() - original_x3.min())\n",
    "    original_x4 = (original_x4 - original_x4.min()) / (original_x4.max() - original_x4.min())\n",
    "    original_x5 = (original_x5 - original_x5.min()) / (original_x5.max() - original_x5.min())\n",
    "    original_x6 = (original_x6 - original_x6.min()) / (original_x6.max() - original_x6.min())\n",
    "    original_x7 = (original_x7 - original_x7.min()) / (original_x7.max() - original_x7.min())\n",
    "    original_x8 = (original_x8 - original_x8.min()) / (original_x8.max() - original_x8.min())\n",
    "    original_x9 = (original_x9 - original_x9.min()) / (original_x9.max() - original_x9.min())\n",
    "    original_x10 = (original_x10 - original_x10.min()) / (original_x10.max() - original_x10.min())\n",
    "    original_x11 = (original_x11 - original_x11.min()) / (original_x11.max() - original_x11.min())\n",
    "    original_x12 = (original_x12 - original_x12.min()) / (original_x12.max() - original_x12.min())\n",
    "    \n",
    "    \n",
    "    # 数据增强\n",
    "    is_dataaug = True\n",
    "    \n",
    "    if is_dataaug:\n",
    "        \n",
    "        rolling_size = 3\n",
    "\n",
    "        aug_x1, aug_x2, aug_x3, aug_x4, aug_x5, aug_x6 = dataaug_rollingavg(original_x1, rolling_size), dataaug_rollingavg(original_x2, rolling_size), dataaug_rollingavg(original_x3, rolling_size), dataaug_rollingavg(original_x4, rolling_size), dataaug_rollingavg(original_x5, rolling_size), dataaug_rollingavg(original_x6, rolling_size)\n",
    "        aug_x7, aug_x8, aug_x9, aug_x10, aug_x11, aug_x12 = dataaug_rollingavg(original_x7, rolling_size), dataaug_rollingavg(original_x8, rolling_size), dataaug_rollingavg(original_x9, rolling_size), dataaug_rollingavg(original_x10, rolling_size), dataaug_rollingavg(original_x11, rolling_size), dataaug_rollingavg(original_x12, rolling_size)\n",
    "        aug_y = dataaug_rollingavg(original_y, rolling_size)\n",
    "\n",
    "        aug_x1, aug_x2, aug_x3, aug_x4, aug_x5, aug_x6 = aug_x1.dropna(), aug_x2.dropna(), aug_x3.dropna(), aug_x4.dropna(), aug_x5.dropna(), aug_x6.dropna()\n",
    "        aug_x7, aug_x8, aug_x9, aug_x10, aug_x11, aug_x12 = aug_x7.dropna(), aug_x8.dropna(), aug_x9.dropna(), aug_x10.dropna(), aug_x11.dropna(), aug_x12.dropna()\n",
    "        aug_y = aug_y.dropna()\n",
    "\n",
    "        original_x1, original_x2 = pd.concat([original_x1, aug_x1], ignore_index=True), pd.concat([original_x2, aug_x2], ignore_index=True)\n",
    "        original_x3, original_x4 = pd.concat([original_x3, aug_x3], ignore_index=True), pd.concat([original_x4, aug_x4], ignore_index=True)\n",
    "        original_x5, original_x6 = pd.concat([original_x5, aug_x5], ignore_index=True), pd.concat([original_x6, aug_x6], ignore_index=True)\n",
    "        original_x7, original_x8 = pd.concat([original_x7, aug_x7], ignore_index=True), pd.concat([original_x8, aug_x8], ignore_index=True)\n",
    "        original_x9, original_x10 = pd.concat([original_x9, aug_x9], ignore_index=True), pd.concat([original_x10, aug_x10], ignore_index=True)\n",
    "        original_x11, original_x12 = pd.concat([original_x11, aug_x11], ignore_index=True), pd.concat([original_x12, aug_x12], ignore_index=True)\n",
    "        original_y = pd.concat([original_y, aug_y], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # # PCA\n",
    "    # pca_fitter = PCA(n_components=10)\n",
    "    # original_x1, original_x2, original_x3, original_x4 = pca_fitter.fit_transform(original_x1), pca_fitter.fit_transform(original_x2), pca_fitter.fit_transform(original_x3), pca_fitter.fit_transform(original_x4)\n",
    "    # original_x5, original_x6, original_x7, original_x8 = pca_fitter.fit_transform(original_x5), pca_fitter.fit_transform(original_x6), pca_fitter.fit_transform(original_x7), pca_fitter.fit_transform(original_x8)\n",
    "    # original_x9, original_x10, original_x11, original_x12 = pca_fitter.fit_transform(original_x9), pca_fitter.fit_transform(original_x10), pca_fitter.fit_transform(original_x11), pca_fitter.fit_transform(original_x12)\n",
    "    \n",
    "    #\n",
    "    # # SELECT-K-BEST\n",
    "    # k = 10  # 选择最相关的特征数量\n",
    "    # selector = SelectKBest(f_classif, k=k)\n",
    "    # original_x1, original_x2, original_x3, original_x4 = selector.fit_transform(original_x1, original_y), selector.fit_transform(original_x2, original_y), selector.fit_transform(original_x3, original_y), selector.fit_transform(original_x4, original_y)\n",
    "\n",
    "\n",
    "    original_x1, original_x2, original_x3, original_x4 = np.array(original_x1), np.array(original_x2), np.array(original_x3), np.array(original_x4)\n",
    "    original_x5, original_x6, original_x7, original_x8 = np.array(original_x5), np.array(original_x6), np.array(original_x7), np.array(original_x8)\n",
    "    original_x9, original_x10 = np.array(original_x9), np.array(original_x10)\n",
    "    original_x11, original_x12 = np.array(original_x11), np.array(original_x12)\n",
    "    original_y = np.array(original_y)\n",
    "\n",
    "    # original_x = np.concatenate([original_x1 + original_x3, original_x2 + original_x4], axis=1)\n",
    "    original_x_thick = (original_x5 + original_x7) * 0.5\n",
    "    original_x_gauscurv = (original_x2 + original_x4) * 0.5\n",
    "    original_x_meancurv = (original_x1 + original_x3) * 0.5\n",
    "    original_x_surfarea = (original_x6 + original_x8) * 0.5\n",
    "    original_x_grayvol = (original_x9 + original_x10) * 0.5\n",
    "    original_x_wmparc = original_x11\n",
    "    original_x_aseg = original_x12\n",
    "    \n",
    "    \n",
    "    \n",
    "    # original_x = np.hstack([original_x_thick, original_x_gauscurv, original_x_meancurv, original_x_surfarea, original_x_grayvol, original_x_wmparc, original_x_aseg])\n",
    "    # original_x = np.hstack([original_x1, original_x2, original_x3, original_x4, original_x5, original_x6, original_x7, original_x8])\n",
    "    original_x = np.hstack([original_x_thick, original_x_gauscurv, original_x_meancurv, original_x_surfarea, original_x_grayvol, original_x_wmparc, original_x_aseg, original_x1, original_x2, original_x3, original_x4, original_x5, original_x6, original_x7, original_x8, original_x9, original_x10])\n",
    "    \n",
    "\n",
    "    return original_x, original_y\n",
    "\n",
    "def data_preprocess():\n",
    "    data_path_1 = r'data/test/lh.MeanCurv- 389.csv'\n",
    "    data_path_2 = r'data/test/lh.GausCurv- 389.csv'\n",
    "    data_path_3 = r'data/test/rh.MeanCurv- 389.csv'\n",
    "    data_path_4 = r'data/test/rh.GausCurv- 389.csv'\n",
    "\n",
    "    data_path_5 = r'data/test/lh.ThickAvg- 389.csv'\n",
    "    data_path_6 = r'data/test/lh.SurfArea - 389.csv'\n",
    "    data_path_7 = r'data/test/rh.ThickAvg- 389.csv'\n",
    "    data_path_8 = r'data/test/rh.SurfArea - 389.csv'\n",
    "    \n",
    "    data_path_9 = r'data/test/lh.GrayVol - 389.csv'\n",
    "    data_path_10 = r'data/test/rh.GrayVol- 389.csv'\n",
    "    \n",
    "    data_path_11 = r'data/test/wmparc - 389.csv'\n",
    "    data_path_12 = r'data/test/aseg - 389.csv'\n",
    "\n",
    "    # label_path = r'data/test/subject_info - 389.csv'\n",
    "\n",
    "    original_x1, original_x2, original_x3, original_x4 = pd.read_csv(data_path_1), pd.read_csv(\n",
    "        data_path_2).dropna(), pd.read_csv(data_path_3), pd.read_csv(data_path_4).dropna()\n",
    "    original_x5, original_x6, original_x7, original_x8 = pd.read_csv(data_path_5), pd.read_csv(\n",
    "        data_path_6), pd.read_csv(data_path_7), pd.read_csv(data_path_8)\n",
    "    original_x9, original_x10 = pd.read_csv(data_path_9), pd.read_csv(data_path_10)\n",
    "    original_x11, original_x12 = pd.read_csv(data_path_11), pd.read_csv(data_path_12)\n",
    "    \n",
    "    original_x1, original_x2, original_x3, original_x4 = original_x1.iloc[:, 1:], original_x2.iloc[:, 1:], original_x3.iloc[:, 1:], original_x4.iloc[:, 1:]\n",
    "    original_x5, original_x6, original_x7, original_x8 = original_x5.iloc[:, 1:], original_x6.iloc[:, 1:], original_x7.iloc[:, 1:], original_x8.iloc[:, 1:]\n",
    "    original_x9, original_x10 = original_x9.iloc[:, 1:], original_x10.iloc[:, 1:]\n",
    "    original_x11, original_x12 = original_x11.iloc[:, 1:], original_x12.iloc[:, 1:]\n",
    "    # original_y = original_y.iloc[:, 3]\n",
    "    \n",
    "    # 目前经验是需要标准化    \n",
    "    original_x1 = (original_x1 - original_x1.min()) / (original_x1.max() - original_x1.min())\n",
    "    original_x2 = (original_x2 - original_x2.min()) / (original_x2.max() - original_x2.min())\n",
    "    original_x3 = (original_x3 - original_x3.min()) / (original_x3.max() - original_x3.min())\n",
    "    original_x4 = (original_x4 - original_x4.min()) / (original_x4.max() - original_x4.min())\n",
    "    original_x5 = (original_x5 - original_x5.min()) / (original_x5.max() - original_x5.min())\n",
    "    original_x6 = (original_x6 - original_x6.min()) / (original_x6.max() - original_x6.min())\n",
    "    original_x7 = (original_x7 - original_x7.min()) / (original_x7.max() - original_x7.min())\n",
    "    original_x8 = (original_x8 - original_x8.min()) / (original_x8.max() - original_x8.min())\n",
    "    original_x9 = (original_x9 - original_x9.min()) / (original_x9.max() - original_x9.min())\n",
    "    original_x10 = (original_x10 - original_x10.min()) / (original_x10.max() - original_x10.min())\n",
    "    original_x11 = (original_x11 - original_x11.min()) / (original_x11.max() - original_x11.min())\n",
    "    original_x12 = (original_x12 - original_x12.min()) / (original_x12.max() - original_x12.min())\n",
    "\n",
    "\n",
    "    original_x1, original_x2, original_x3, original_x4 = np.array(original_x1), np.array(original_x2), np.array(original_x3), np.array(original_x4)\n",
    "    original_x5, original_x6, original_x7, original_x8 = np.array(original_x5), np.array(original_x6), np.array(original_x7), np.array(original_x8)\n",
    "    original_x9, original_x10 = np.array(original_x9), np.array(original_x10)\n",
    "    original_x11, original_x12 = np.array(original_x11), np.array(original_x12)\n",
    "    # original_y = np.array(original_y)\n",
    "\n",
    "    # original_x = np.concatenate([original_x1 + original_x3, original_x2 + original_x4], axis=1)\n",
    "    original_x_thick = (original_x5 + original_x7) * 0.5\n",
    "    original_x_gauscurv = (original_x2 + original_x4) * 0.5\n",
    "    original_x_meancurv = (original_x1 + original_x3) * 0.5\n",
    "    original_x_surfarea = (original_x6 + original_x8) * 0.5\n",
    "    original_x_grayvol = (original_x9 + original_x10) * 0.5\n",
    "    original_x_wmparc = original_x11\n",
    "    original_x_aseg = original_x12\n",
    "    \n",
    "    \n",
    "    \n",
    "    # original_x = np.hstack([original_x_thick, original_x_gauscurv, original_x_meancurv, original_x_surfarea, original_x_grayvol, original_x_wmparc, original_x_aseg])\n",
    "    # original_x = np.hstack([original_x1, original_x2, original_x3, original_x4, original_x5, original_x6, original_x7, original_x8])\n",
    "    original_x = np.hstack([original_x_thick, original_x_gauscurv, original_x_meancurv, original_x_surfarea, original_x_grayvol, original_x_wmparc, original_x_aseg, original_x1, original_x2, original_x3, original_x4, original_x5, original_x6, original_x7, original_x8, original_x9, original_x10])\n",
    "    \n",
    "\n",
    "    return original_x\n",
    "\n",
    "def write_submission(prediction_list, submit_name):\n",
    "    \n",
    "    ori_submission_path = r'subject_info - 389.csv'\n",
    "    \n",
    "    predict_df = pd.read_csv(ori_submission_path, encoding='gbk')\n",
    "    predict_df['年龄'] = prediction_list\n",
    "    \n",
    "    print('\\nSumission CSV Preview')\n",
    "    print(predict_df.head(10))\n",
    "    predict_df.to_csv(submit_name + '.csv')\n",
    "\n",
    "    \n",
    "    print('Successfully Build a Submission CSV File')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc3277-1bfa-4836-828c-1540f7ad32ff",
   "metadata": {},
   "source": [
    "## ***Section II: MachingLearningTraining***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e137c7bc-357a-437e-aed4-4d87e25e8299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting lightgbm\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f6/9d/fae632fd823b407448b9cd2b28288172c040415e2c9ab401cca9e67b4192/lightgbm-4.0.0-py3-none-manylinux_2_28_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 693 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in ./miniconda3/lib/python3.8/site-packages (from lightgbm) (1.10.1)\n",
      "Requirement already satisfied: numpy in ./miniconda3/lib/python3.8/site-packages (from lightgbm) (1.22.4)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a2be63b-1ab4-4ec7-97e9-88b24fd00cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_768/3313369794.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_valid = torch.tensor(X_train, dtype=torch.float32)\n",
      "/tmp/ipykernel_768/3313369794.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_valid = torch.tensor(y_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RVR Model\n",
      "\n",
      "VALID MAE:9.818588025022342\n",
      "VALID Variance 2382.270549873379\n",
      "[-9, 15, -17, 17, 8, -11, 21, 3, 11, 22, 1, -15, 5, -18, 13, 5, -11, 33, 15, -4, 13, 14, 32, 16, 11, 12, 4, 1, 0, 12, 28, 21, 11, 39, 59, 49, 57, 107, 36, 21, -8, 0, -5, 17, 0, 11, 32, 16, 26, 34, 33, 46, -11, -15, 25, 2, -15, -2, 10, 8, 8, 44, -4, 1, 11, 1, -39, -5, -6, 1, 15, 39, -8, 34, 35, 1, 31, 29, 39, 14, 31, 13, 13, 15, -14, 32, 14, 34, 34, 38, 34, 48, 13, 35, 6, -31, 10, 0, 20, 39, 14, 62, 37, 10, 22, -6, 12, 21, 14, 4, 17, 6, -1, 37, 3, 5, 12, 7, 12, 15, 6, 4, 37, 46, -11, 12, 32, 22, 14, 11, 25, 45, 62, 30, 10, -17, 60, 29, 18, 33, 20, 23, 34, 34, 27, 17, 17, 22, 1, 24, 4, -10, -21, 23, 8, 28, 15, 23, -39, 15, -28, 0, 36, -10, 47, 30, -5, 13, 28, -14, -22, -22, -1, -4, 5, 0, 21, -1, 12, -9, 16, -28, -14, 14, -5, -17, 41, 23, 31, 68, 46, 26, 5, 26, 23, 19, 9, -16, -3, -10, 19, 9, -6, 3, -7, 4, -4, -20, 32, -1, 28, -1, -35, 1, 0, -20, 6, 38, 6, 14, 6, 23, 30, 55, 27, 42, 17, 33, 51, 44, 10, 30, 44, 22, 22, 57, 36, -8, 7, 14, 13, 39, 33, 42, -2, 8, 7, 16, -5, -8, 21, 29, -19, 6, 10, 52, -25, 9, 13, 42, 18, 17, 47, 36, 35, 25, 41, 24, 47, 46, 39, 50, 22, 13, 34, 10, 36, 17, 7, 5, -10, 19, 40, 7, 17, 19, 47, -10, -18, 14, -5, 9, -40, 30, 37, 41, 4, 28, -3, 13, -3, 1, 25, 54, 64, 33, 5, 0, 16, 27, -15, -20, 0, 12, -22, -3, 25, 16, 35, 9, 14, 7, 6, 91, 30, -7, 13, 10, -4, 13, 35, 16, 39, 39, 21, 19, 37, 18, 28, 39, 19, 10, 38, 37, 46, 7, 62, 54, 1, 24, 29, 49, 0, 6, 44, 49, 26, 18, 28, 5, 16, 31, 27, 55, 45, 26, 49, 40, 57, 58, 47, 3, 7, 10, -8, 11, -5, 25, 17, 1, 0, 16, 0, 19, 7, 16, 3, 3, 10]\n",
      "TEST Variance 440.0626482775028\n",
      "\n",
      "VALID MAE:9.986595174262735\n",
      "VALID Variance 2373.443779194529\n",
      "[23, 26, 12, 31, 33, 24, 29, 26, 26, 29, 25, 14, 23, 24, 31, 31, 17, 39, 31, 25, 22, 33, 33, 38, 28, 34, 21, 17, 21, 27, 30, 31, 34, 43, 43, 40, 37, 53, 27, 29, 27, 34, 32, 44, 34, 31, 37, 33, 40, 40, 41, 56, 30, 27, 43, 31, 26, 31, 38, 37, 35, 44, 33, 22, 40, 27, 16, 28, 23, 30, 36, 49, 23, 54, 44, 27, 46, 44, 37, 31, 32, 34, 30, 37, 16, 31, 31, 41, 41, 44, 43, 52, 27, 42, 36, 11, 31, 24, 38, 47, 33, 54, 45, 28, 36, 23, 30, 22, 31, 32, 40, 41, 33, 46, 32, 31, 38, 33, 38, 42, 33, 29, 36, 33, 21, 32, 34, 35, 36, 31, 42, 44, 53, 35, 34, 14, 45, 41, 33, 37, 37, 30, 41, 43, 37, 34, 32, 32, 20, 36, 24, 30, 13, 31, 33, 38, 39, 38, 7, 26, 17, 30, 37, 17, 47, 35, 10, 26, 41, 17, 14, 19, 21, 24, 29, 33, 38, 33, 34, 31, 35, 16, 22, 43, 23, 17, 46, 33, 36, 59, 48, 44, 25, 40, 39, 30, 36, 19, 21, 24, 33, 35, 16, 32, 30, 30, 25, 15, 31, 26, 42, 16, 8, 20, 24, 12, 28, 29, 26, 27, 24, 38, 33, 48, 36, 40, 35, 38, 46, 44, 33, 35, 42, 39, 39, 46, 44, 25, 32, 31, 28, 39, 39, 43, 19, 17, 22, 30, 20, 14, 32, 31, 15, 23, 21, 53, 16, 33, 33, 47, 38, 34, 44, 38, 48, 38, 38, 35, 42, 49, 48, 61, 33, 32, 48, 25, 36, 34, 28, 26, 16, 33, 41, 21, 26, 39, 43, 21, 12, 27, 22, 40, 4, 41, 44, 40, 23, 34, 31, 34, 23, 35, 42, 52, 55, 46, 26, 22, 35, 36, 14, 10, 26, 34, 20, 18, 33, 30, 39, 29, 26, 35, 27, 49, 43, 25, 39, 29, 21, 30, 48, 39, 42, 44, 38, 34, 40, 37, 42, 43, 30, 25, 45, 44, 45, 33, 63, 42, 22, 42, 33, 40, 22, 22, 52, 43, 37, 33, 31, 32, 29, 36, 41, 50, 44, 39, 45, 42, 47, 41, 46, 31, 29, 27, 29, 30, 32, 37, 32, 27, 27, 34, 27, 32, 28, 37, 28, 26, 28]\n",
      "TEST Variance 96.1168905835938\n",
      "\n",
      "VALID MAE:11.498212689901697\n",
      "VALID Variance 2337.781153741412\n",
      "[23, 26, 12, 31, 33, 24, 29, 26, 26, 29, 25, 14, 23, 24, 31, 31, 17, 39, 31, 25, 22, 33, 33, 38, 28, 34, 21, 17, 21, 27, 30, 31, 34, 43, 43, 40, 37, 53, 27, 29, 27, 34, 32, 44, 34, 31, 37, 33, 40, 40, 41, 56, 30, 27, 43, 31, 26, 31, 38, 37, 35, 44, 33, 22, 40, 27, 16, 28, 23, 30, 36, 49, 23, 54, 44, 27, 46, 44, 37, 31, 32, 34, 30, 37, 16, 31, 31, 41, 41, 44, 43, 52, 27, 42, 36, 11, 31, 24, 38, 47, 33, 54, 45, 28, 36, 23, 30, 22, 31, 32, 40, 41, 33, 46, 32, 31, 38, 33, 38, 42, 33, 29, 36, 33, 21, 32, 34, 35, 36, 31, 42, 44, 53, 35, 34, 14, 45, 41, 33, 37, 37, 30, 41, 43, 37, 34, 32, 32, 20, 36, 24, 30, 13, 31, 33, 38, 39, 38, 7, 26, 17, 30, 37, 17, 47, 35, 10, 26, 41, 17, 14, 19, 21, 24, 29, 33, 38, 33, 34, 31, 35, 16, 22, 43, 23, 17, 46, 33, 36, 59, 48, 44, 25, 40, 39, 30, 36, 19, 21, 24, 33, 35, 16, 32, 30, 30, 25, 15, 31, 26, 42, 16, 8, 20, 24, 12, 28, 29, 26, 27, 24, 38, 33, 48, 36, 40, 35, 38, 46, 44, 33, 35, 42, 39, 39, 46, 44, 25, 32, 31, 28, 39, 39, 43, 19, 17, 22, 30, 20, 14, 32, 31, 15, 23, 21, 53, 16, 33, 33, 47, 38, 34, 44, 38, 48, 38, 38, 35, 42, 49, 48, 61, 33, 32, 48, 25, 36, 34, 28, 26, 16, 33, 41, 21, 26, 39, 43, 21, 12, 27, 22, 40, 4, 41, 44, 40, 23, 34, 31, 34, 23, 35, 42, 52, 55, 46, 26, 22, 35, 36, 14, 10, 26, 34, 20, 18, 33, 30, 39, 29, 26, 35, 27, 49, 43, 25, 39, 29, 21, 30, 48, 39, 42, 44, 38, 34, 40, 37, 42, 43, 30, 25, 45, 44, 45, 33, 63, 42, 22, 42, 33, 40, 22, 22, 52, 43, 37, 33, 31, 32, 29, 36, 41, 50, 44, 39, 45, 42, 47, 41, 46, 31, 29, 27, 29, 30, 32, 37, 32, 27, 27, 34, 27, 32, 28, 37, 28, 26, 28]\n",
      "TEST Variance 96.1168905835938\n",
      "\n",
      " RandomForest Model\n",
      "\n",
      "VALID MAE:5.518766756032171\n",
      "VALID Variance 2350.5073055701305\n",
      "[42, 39, 42, 43, 43, 39, 37, 49, 40, 45, 45, 39, 48, 51, 47, 43, 40, 45, 39, 45, 44, 49, 43, 41, 46, 46, 45, 47, 41, 44, 37, 43, 36, 49, 46, 42, 42, 46, 34, 48, 45, 50, 43, 47, 51, 47, 46, 49, 46, 45, 48, 48, 50, 50, 45, 53, 52, 43, 46, 48, 45, 48, 47, 40, 50, 50, 47, 36, 43, 38, 45, 43, 49, 46, 52, 44, 44, 50, 47, 43, 44, 45, 41, 34, 42, 41, 47, 38, 44, 43, 44, 47, 37, 43, 48, 35, 51, 47, 41, 48, 48, 45, 50, 45, 52, 42, 47, 44, 46, 47, 41, 47, 41, 46, 44, 43, 49, 51, 52, 51, 46, 48, 43, 39, 37, 41, 44, 47, 42, 47, 43, 49, 47, 50, 41, 39, 38, 47, 47, 48, 45, 41, 43, 52, 42, 43, 44, 47, 45, 48, 47, 53, 40, 44, 49, 43, 37, 38, 44, 37, 42, 41, 41, 47, 45, 42, 37, 46, 46, 42, 46, 43, 40, 50, 44, 54, 51, 48, 45, 51, 48, 53, 46, 47, 42, 49, 46, 46, 41, 48, 47, 49, 45, 48, 44, 46, 45, 44, 44, 49, 45, 43, 47, 40, 46, 44, 54, 39, 52, 47, 45, 38, 45, 47, 48, 43, 50, 39, 41, 46, 42, 51, 46, 46, 49, 47, 44, 46, 38, 41, 47, 43, 40, 50, 43, 47, 49, 42, 51, 45, 47, 47, 46, 47, 43, 52, 42, 47, 48, 44, 44, 39, 41, 48, 48, 41, 43, 46, 45, 47, 41, 44, 45, 42, 49, 43, 41, 48, 43, 48, 50, 48, 48, 45, 46, 44, 42, 42, 47, 36, 44, 41, 46, 45, 38, 48, 43, 49, 45, 50, 56, 54, 33, 44, 41, 48, 48, 49, 44, 45, 44, 53, 42, 40, 38, 51, 45, 50, 49, 46, 41, 46, 46, 51, 40, 40, 53, 47, 50, 42, 44, 46, 42, 44, 45, 48, 54, 41, 41, 44, 48, 49, 52, 49, 46, 38, 50, 46, 48, 46, 46, 44, 49, 44, 42, 47, 52, 46, 44, 46, 39, 39, 44, 44, 48, 42, 44, 52, 42, 50, 36, 44, 49, 43, 37, 47, 45, 50, 52, 43, 42, 45, 42, 43, 41, 50, 51, 49, 46, 49, 47, 46, 41, 40, 44, 46, 44, 43, 52]\n",
      "TEST Variance 16.329009192379115\n",
      "\n",
      " Bagging Model\n",
      "\n",
      "VALID MAE:5.570151921358356\n",
      "VALID Variance 2357.872891744752\n",
      "[42, 34, 34, 46, 47, 41, 49, 50, 38, 47, 33, 48, 40, 38, 50, 47, 37, 47, 44, 51, 48, 55, 49, 43, 37, 46, 46, 36, 37, 45, 48, 40, 43, 45, 50, 46, 43, 48, 33, 53, 37, 46, 48, 45, 51, 44, 47, 43, 53, 46, 43, 46, 43, 44, 45, 45, 42, 44, 49, 54, 46, 46, 49, 40, 46, 39, 48, 44, 48, 39, 53, 42, 46, 48, 51, 43, 48, 48, 41, 43, 53, 39, 39, 49, 31, 46, 53, 53, 46, 45, 51, 47, 40, 52, 42, 40, 44, 49, 46, 43, 50, 55, 47, 45, 43, 44, 35, 41, 46, 49, 49, 45, 45, 46, 44, 41, 44, 51, 53, 46, 52, 42, 52, 38, 36, 41, 41, 50, 44, 50, 44, 45, 47, 37, 43, 46, 46, 54, 38, 45, 51, 36, 45, 51, 48, 47, 40, 43, 43, 42, 45, 43, 44, 46, 44, 49, 47, 49, 39, 48, 45, 43, 47, 42, 38, 46, 37, 36, 52, 39, 46, 39, 45, 47, 43, 48, 48, 43, 50, 48, 43, 44, 33, 48, 38, 35, 44, 43, 43, 43, 50, 50, 40, 41, 44, 44, 44, 43, 40, 39, 48, 43, 46, 50, 50, 38, 49, 40, 41, 45, 51, 38, 36, 49, 37, 34, 42, 44, 50, 43, 48, 44, 49, 51, 49, 51, 39, 48, 46, 45, 47, 48, 45, 47, 40, 49, 51, 40, 43, 45, 46, 51, 44, 40, 48, 43, 45, 52, 43, 39, 45, 43, 41, 41, 36, 55, 38, 40, 41, 44, 45, 41, 47, 43, 43, 45, 52, 49, 38, 54, 43, 46, 50, 50, 54, 47, 48, 41, 41, 42, 38, 44, 44, 42, 46, 43, 45, 35, 42, 53, 35, 43, 37, 40, 45, 43, 37, 42, 50, 42, 37, 37, 47, 51, 44, 45, 49, 50, 39, 41, 39, 45, 41, 48, 46, 42, 49, 45, 46, 49, 44, 49, 43, 27, 42, 52, 49, 40, 32, 36, 47, 55, 53, 48, 44, 44, 51, 45, 37, 42, 42, 41, 47, 49, 42, 50, 53, 45, 41, 45, 46, 49, 43, 41, 45, 40, 44, 46, 44, 45, 42, 45, 45, 42, 39, 45, 43, 48, 45, 46, 51, 47, 47, 47, 49, 46, 43, 50, 45, 47, 43, 51, 44, 52, 48, 49, 52, 44, 34]\n",
      "TEST Variance 23.02424646942593\n",
      "\n",
      " XGBoost Model\n",
      "\n",
      "VALID MAE:0.8078641644325291\n",
      "LABEL Variance 2552.5675021023653\n",
      "VALID Variance 2532.760557148801\n",
      "[38, 51, 25, 48, 38, 39, 40, 43, 32, 46, 50, 44, 56, 40, 38, 39, 38, 40, 50, 53, 41, 48, 40, 43, 51, 45, 38, 32, 34, 48, 41, 27, 53, 50, 52, 41, 63, 39, 44, 47, 35, 50, 47, 44, 54, 42, 52, 37, 56, 46, 88, 56, 41, 42, 46, 48, 44, 47, 42, 46, 47, 50, 53, 81, 51, 47, 43, 42, 44, 36, 48, 43, 82, 48, 58, 44, 44, 46, 43, 46, 49, 45, 43, 43, 26, 44, 51, 41, 51, 47, 42, 44, 35, 56, 47, 25, 48, 44, 52, 56, 51, 49, 49, 47, 47, 24, 53, 39, 41, 45, 53, 48, 44, 54, 42, 85, 49, 50, 46, 52, 46, 45, 51, 53, 29, 35, 43, 48, 44, 46, 51, 60, 50, 54, 51, 42, 51, 58, 45, 43, 49, 43, 48, 54, 51, 52, 43, 52, 33, 38, 44, 52, 51, 55, 43, 41, 54, 43, 39, 48, 36, 44, 43, 44, 53, 47, 32, 45, 52, 42, 44, 48, 47, 87, 44, 53, 59, 47, 50, 38, 56, 36, 49, 53, 44, 32, 55, 50, 50, 53, 43, 50, 49, 53, 59, 51, 45, 49, 53, 38, 40, 44, 54, 39, 46, 45, 55, 46, 44, 48, 45, 33, 27, 46, 44, 82, 48, 41, 39, 39, 46, 50, 43, 46, 37, 51, 40, 52, 42, 49, 45, 91, 54, 56, 50, 50, 57, 44, 46, 40, 42, 50, 53, 53, 25, 44, 43, 43, 32, 33, 47, 40, 39, 40, 44, 56, 27, 43, 43, 58, 49, 52, 48, 47, 50, 55, 43, 31, 40, 51, 51, 54, 49, 46, 59, 47, 49, 33, 51, 42, 41, 36, 57, 43, 51, 50, 52, 40, 32, 53, 35, 51, 32, 52, 53, 61, 29, 44, 54, 41, 44, 47, 51, 45, 46, 53, 33, 47, 46, 45, 37, 41, 44, 40, 45, 48, 49, 46, 57, 52, 45, 41, 42, 40, 52, 48, 59, 39, 46, 42, 54, 45, 45, 50, 54, 50, 61, 52, 52, 56, 37, 28, 56, 60, 40, 46, 53, 51, 33, 51, 44, 46, 38, 39, 53, 50, 58, 45, 40, 57, 42, 48, 52, 55, 50, 59, 57, 60, 64, 51, 56, 42, 53, 42, 38, 46, 51, 53, 50, 38, 43, 52, 50, 45, 46, 45, 50, 50, 36]\n",
      "TEST Variance 77.87736004916701\n",
      "\n",
      " LightGBM Model\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 136407\n",
      "[LightGBM] [Info] Number of data points in the train set: 2238, number of used features: 562\n",
      "[LightGBM] [Info] Start training from score 87.597855\n",
      "\n",
      "VALID MAE:2.6193029490616624\n",
      "LABEL Variance 2552.5675021023653\n",
      "VALID Variance 2472.9751094308162\n",
      "[35, 43, 36, 48, 48, 38, 46, 46, 41, 49, 45, 53, 46, 40, 43, 47, 42, 47, 52, 43, 44, 50, 52, 52, 47, 42, 45, 40, 41, 41, 46, 42, 55, 54, 47, 43, 65, 53, 43, 44, 49, 53, 46, 53, 51, 44, 53, 44, 53, 46, 55, 60, 46, 46, 46, 49, 45, 44, 48, 54, 52, 53, 49, 43, 48, 46, 43, 53, 49, 41, 55, 52, 45, 54, 53, 49, 46, 58, 49, 50, 51, 47, 48, 52, 34, 48, 52, 46, 55, 50, 55, 55, 43, 49, 49, 35, 41, 49, 50, 52, 43, 51, 55, 50, 48, 39, 46, 47, 49, 51, 60, 46, 41, 54, 50, 47, 52, 49, 56, 51, 48, 51, 50, 52, 40, 42, 48, 44, 46, 51, 50, 60, 53, 55, 45, 48, 55, 57, 48, 49, 54, 43, 51, 52, 48, 55, 49, 55, 43, 47, 43, 45, 40, 52, 47, 48, 51, 48, 36, 48, 36, 47, 44, 44, 52, 49, 34, 46, 53, 39, 51, 42, 45, 43, 49, 47, 51, 49, 54, 47, 56, 42, 45, 54, 40, 40, 55, 42, 52, 55, 53, 51, 42, 49, 54, 48, 42, 47, 47, 46, 50, 42, 51, 49, 46, 49, 51, 40, 46, 45, 51, 38, 33, 41, 44, 30, 46, 36, 42, 42, 43, 45, 45, 53, 45, 57, 43, 47, 43, 42, 46, 47, 51, 52, 50, 56, 49, 44, 51, 42, 45, 54, 53, 48, 39, 44, 45, 44, 42, 42, 50, 39, 32, 36, 37, 60, 33, 50, 41, 50, 53, 51, 52, 50, 55, 52, 45, 44, 50, 55, 57, 56, 48, 49, 55, 47, 48, 42, 54, 45, 40, 43, 52, 46, 47, 49, 49, 42, 33, 48, 37, 51, 34, 53, 46, 51, 42, 42, 53, 48, 51, 47, 53, 53, 50, 54, 48, 46, 52, 51, 39, 33, 47, 48, 46, 42, 50, 52, 52, 50, 47, 54, 44, 47, 47, 42, 54, 38, 38, 42, 53, 51, 49, 50, 52, 48, 53, 56, 53, 47, 45, 44, 53, 53, 51, 48, 54, 51, 39, 48, 43, 54, 37, 39, 55, 45, 49, 45, 41, 57, 45, 50, 54, 52, 51, 51, 58, 54, 60, 46, 55, 46, 48, 46, 51, 42, 47, 50, 43, 45, 48, 51, 45, 48, 49, 55, 49, 53, 48]\n",
      "TEST Variance 30.92012344618394\n",
      "\n",
      " Ensemble Model\n",
      "\n",
      "ENSEMBLE VALID MAE:4.837056896038129\n",
      "ENSEMBLE VALID Variance 2388.374510146858\n",
      "[23, 33, 14, 37, 31, 23, 36, 32, 27, 38, 28, 25, 33, 20, 33, 30, 21, 40, 36, 33, 34, 39, 40, 34, 33, 34, 29, 23, 23, 35, 39, 29, 35, 44, 53, 45, 54, 64, 37, 40, 21, 32, 30, 35, 35, 32, 43, 32, 45, 42, 54, 49, 24, 23, 38, 31, 23, 29, 33, 36, 33, 46, 32, 40, 36, 29, 17, 27, 28, 25, 38, 41, 40, 43, 48, 29, 41, 41, 41, 34, 44, 32, 31, 35, 14, 40, 39, 42, 43, 43, 42, 46, 29, 47, 31, 11, 34, 31, 39, 46, 38, 55, 44, 34, 37, 20, 33, 33, 33, 32, 39, 33, 29, 45, 29, 43, 35, 36, 37, 37, 34, 30, 46, 45, 18, 29, 38, 40, 34, 35, 40, 50, 53, 40, 34, 23, 52, 47, 33, 40, 40, 34, 42, 46, 42, 38, 33, 39, 25, 34, 31, 28, 24, 41, 31, 39, 38, 38, 13, 37, 17, 29, 42, 25, 46, 41, 21, 31, 44, 22, 22, 21, 30, 43, 30, 33, 42, 29, 37, 25, 38, 17, 22, 38, 25, 16, 46, 38, 41, 54, 46, 42, 31, 40, 42, 38, 32, 25, 30, 22, 35, 32, 31, 30, 29, 29, 33, 22, 39, 30, 41, 23, 9, 32, 27, 32, 32, 41, 31, 32, 33, 39, 40, 50, 37, 48, 32, 44, 46, 46, 34, 56, 47, 41, 37, 52, 48, 25, 32, 33, 33, 46, 43, 45, 23, 31, 31, 37, 23, 21, 37, 37, 20, 29, 30, 54, 13, 30, 32, 48, 37, 36, 47, 42, 42, 41, 45, 34, 41, 50, 44, 50, 40, 36, 49, 34, 44, 30, 33, 29, 23, 33, 47, 30, 38, 37, 48, 21, 18, 40, 21, 34, 9, 40, 45, 48, 23, 38, 33, 32, 26, 28, 41, 50, 51, 43, 29, 32, 33, 37, 20, 22, 28, 33, 23, 29, 41, 35, 46, 36, 34, 32, 30, 52, 41, 31, 40, 29, 24, 30, 45, 38, 45, 45, 39, 37, 49, 38, 39, 45, 32, 26, 47, 48, 42, 34, 56, 50, 25, 40, 39, 48, 27, 28, 47, 46, 42, 36, 37, 35, 33, 41, 41, 50, 44, 43, 49, 49, 55, 51, 51, 30, 35, 33, 26, 34, 29, 42, 37, 28, 28, 39, 31, 38, 33, 36, 35, 32, 26]\n",
      "TEST Variance 81.7681881563035\n",
      "\n",
      "Sumission CSV Preview\n",
      "   subject_ID MRI扫描仪类型  性别  注：性别（1=Female, 2=Male)  年龄\n",
      "0  CNBD_00011  SIEMENS   2                     NaN  35\n",
      "1  CNBD_00014  SIEMENS   1                     NaN  43\n",
      "2  CNBD_00015  SIEMENS   2                     NaN  36\n",
      "3  CNBD_00029  SIEMENS   2                     NaN  48\n",
      "4  CNBD_00046  SIEMENS   1                     NaN  48\n",
      "5  CNBD_00053  SIEMENS   2                     NaN  38\n",
      "6  CNBD_00058  SIEMENS   2                     NaN  46\n",
      "7  CNBD_00061  SIEMENS   1                     NaN  46\n",
      "8  CNBD_00062  SIEMENS   2                     NaN  41\n",
      "9  CNBD_00064  SIEMENS   1                     NaN  49\n",
      "Successfully Build a Submission CSV File\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def rvm_prediction():\n",
    "    train_original_x, train_original_y = train_data_preprocess()\n",
    "    test_original_x = data_preprocess()\n",
    "\n",
    "    # 将数据集分为训练集和测试集\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train_original_x, train_original_y, test_size=0.3, random_state=0)\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_valid = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_valid = torch.tensor(y_train, dtype=torch.float32)\n",
    "    test_original_x = torch.tensor(test_original_x, dtype=torch.float32)\n",
    "    \n",
    "    # RVR\n",
    "    print('\\n RVR Model')\n",
    "    \n",
    "    linear_model = RVR(kernel=\"linear\")\n",
    "    linear_model.fit(X_train, y_train)\n",
    "    linear_model_valid_prediction = linear_model.predict(X_valid)\n",
    "    print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in linear_model_valid_prediction])))\n",
    "    print('VALID Variance {}'.format(np.var([int(i) for i in linear_model_valid_prediction])))\n",
    "    linear_model_test_prediction = linear_model.predict(test_original_x)\n",
    "    print([int(i) for i in linear_model_test_prediction])\n",
    "    print('TEST Variance {}'.format(np.var([int(i) for i in linear_model_test_prediction])))\n",
    "\n",
    "    rbf_model = RVR(kernel=\"rbf\")\n",
    "    rbf_model.fit(X_train, y_train)\n",
    "    rbf_model_valid_prediction = rbf_model.predict(X_valid)\n",
    "    print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in rbf_model_valid_prediction])))\n",
    "    print('VALID Variance {}'.format(np.var([int(i) for i in rbf_model_valid_prediction])))\n",
    "    rbf_model_test_prediction = rbf_model.predict(test_original_x)\n",
    "    print([int(i) for i in rbf_model_test_prediction])\n",
    "    print('TEST Variance {}'.format(np.var([int(i) for i in rbf_model_test_prediction])))\n",
    "\n",
    "    poly_model = RVR(kernel=\"poly\")\n",
    "    poly_model.fit(X_train, y_train)\n",
    "    poly_model_valid_prediction = poly_model.predict(X_valid)\n",
    "    print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in poly_model_valid_prediction])))\n",
    "    print('VALID Variance {}'.format(np.var([int(i) for i in poly_model_valid_prediction])))\n",
    "    poly_model_test_prediction = rbf_model.predict(test_original_x)\n",
    "    print([int(i) for i in poly_model_test_prediction])\n",
    "    print('TEST Variance {}'.format(np.var([int(i) for i in poly_model_test_prediction])))\n",
    "    \n",
    "    # 随机森林\n",
    "    print('\\n RandomForest Model')\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=12,random_state=0)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_model_valid_prediction = rf_model.predict(X_valid)\n",
    "    print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in rf_model_valid_prediction])))\n",
    "    print('VALID Variance {}'.format(np.var([int(i) for i in rf_model_valid_prediction])))\n",
    "    rf_model_test_prediction = rf_model.predict(test_original_x)\n",
    "    print([int(i) for i in rf_model_test_prediction])\n",
    "    print('TEST Variance {}'.format(np.var([int(i) for i in rf_model_test_prediction])))\n",
    "    \n",
    "    # Bagging\n",
    "    print('\\n Bagging Model')\n",
    "    \n",
    "    bag_model = BaggingRegressor()\n",
    "    bag_model.fit(X_train, y_train)\n",
    "    bag_model_valid_prediction = bag_model.predict(X_valid)\n",
    "    print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in bag_model_valid_prediction])))\n",
    "    print('VALID Variance {}'.format(np.var([int(i) for i in bag_model_valid_prediction])))\n",
    "    bag_model_test_prediction = bag_model.predict(test_original_x)\n",
    "    print([int(i) for i in bag_model_test_prediction])\n",
    "    print('TEST Variance {}'.format(np.var([int(i) for i in bag_model_test_prediction])))\n",
    "    \n",
    "    # XGBoost\n",
    "    print('\\n XGBoost Model')\n",
    "    \n",
    "    xgb_model = XGBRegressor(learning_rate=0.2)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_model_valid_prediction = xgb_model.predict(X_valid)\n",
    "    print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in xgb_model_valid_prediction])))\n",
    "    print('LABEL Variance {}'.format(np.var([int(i) for i in y_valid])))\n",
    "    print('VALID Variance {}'.format(np.var([int(i) for i in xgb_model_valid_prediction])))\n",
    "    xgb_model_test_prediction = xgb_model.predict(test_original_x)\n",
    "    print([int(i) for i in xgb_model_test_prediction])\n",
    "    print('TEST Variance {}'.format(np.var([int(i) for i in xgb_model_test_prediction])))\n",
    "    \n",
    "    # LightGBM\n",
    "    print('\\n LightGBM Model')\n",
    "    \n",
    "    lgbm_model = LGBMRegressor()\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "    lgbm_model_valid_prediction = lgbm_model.predict(X_valid)\n",
    "    print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in lgbm_model_valid_prediction])))\n",
    "    print('LABEL Variance {}'.format(np.var([int(i) for i in y_valid])))\n",
    "    print('VALID Variance {}'.format(np.var([int(i) for i in lgbm_model_valid_prediction])))\n",
    "    lgbm_model_test_prediction = lgbm_model.predict(test_original_x)\n",
    "    print([int(i) for i in lgbm_model_test_prediction])\n",
    "    print('TEST Variance {}'.format(np.var([int(i) for i in lgbm_model_test_prediction])))\n",
    "    \n",
    "    \n",
    "    # 集成模型\n",
    "    print('\\n Ensemble Model')\n",
    "    \n",
    "    new_linear_pred, new_rbf_pred, new_poly_pred = [int(i) for i in linear_model_valid_prediction], [int(i) for i in rbf_model_valid_prediction], [int(i) for i in poly_model_valid_prediction]\n",
    "    new_rf_pred = [int(i) for i in rf_model_valid_prediction]\n",
    "    new_bag_pred = [int(i) for i in bag_model_valid_prediction]   \n",
    "    new_xgb_pred = [int(i) for i in xgb_model_valid_prediction] \n",
    "    new_lgbm_pred = [int(i) for i in lgbm_model_valid_prediction] \n",
    "    \n",
    "    # ensemble_prediction = [(i + j) / 2 for i, j in zip(new_rf_pred, new_rbf_pred)]\n",
    "    ensemble_prediction = [(i + j + k) / 3 for i, j, k in zip(new_xgb_pred, new_bag_pred, new_linear_pred)]\n",
    "    print('\\nENSEMBLE VALID MAE:{}'.format(mean_absolute_error(y_valid, ensemble_prediction)))\n",
    "    print('ENSEMBLE VALID Variance {}'.format(np.var([int(i) for i in ensemble_prediction])))\n",
    "    \n",
    "    test_new_linear_pred, test_new_rbf_pred, test_new_poly_pred = [int(i) for i in linear_model_test_prediction], [int(i) for i in rbf_model_test_prediction], [int(i) for i in poly_model_test_prediction]\n",
    "    test_new_bag_pred = [int(i) for i in bag_model_test_prediction]\n",
    "    test_new_rf_pred = [int(i) for i in rf_model_test_prediction]\n",
    "    test_new_xgb_pref = [int(i) for i in xgb_model_test_prediction]\n",
    "    \n",
    "    # test_ensemble_prediction = [(i + j) / 2 for i, j in zip(test_new_rf_pred, test_new_rbf_pred)]\n",
    "    test_ensemble_prediction = [(i + j + k) / 3 for i, j, k in zip(test_new_xgb_pref, test_new_bag_pred, test_new_linear_pred)]\n",
    "    print([int(i) for i in test_ensemble_prediction])\n",
    "    print('TEST Variance {}'.format(np.var([int(i) for i in test_ensemble_prediction])))\n",
    "    \n",
    "    \n",
    "    # 写结果\n",
    "    final_list = [int(i) for i in [int(i) for i in lgbm_model_test_prediction]]\n",
    "    submission_version_name = 'Ensemble-DataAug3-XGBoost_Bagging_RVR'\n",
    "    write_submission(final_list, submission_version_name)\n",
    "\n",
    "    \n",
    "rvm_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e25ae8-4b46-4ceb-862e-135ca8922c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45 44 38 64 22 24 27 32 66 24 68 64 54 34 39 23 63 33 61 30 52 18 71 37\n",
      " 27 48 37 58 36 69 62 39 45 49 46 32 40 35 34 46 32 61 54 62 26 56 36 67\n",
      " 62 53 68 34 45 24 53 45 56 21 25 47 63 32 28 43 38 39 29 27 44 67 22 53\n",
      " 41 52 21 54 34 21 26 27 43 25 22 69 52 54 51 60 27 22 61 24 49 45 38 60\n",
      " 34 61 51 29 47 40 55 23 20 38 28 42 24 52 58 34 38 47 46 65 45 26 54 63\n",
      " 69 53 38 67 25 57 69 57 62 60 28 27 66 31 45 27 62 23 43 61 38 21 40 41\n",
      " 21 40 22 55 35 22 58 37 65 38 25 31 35 31 56 47 40 20 33 25 67 59 44 69\n",
      " 46 69 39 51 44 38 33 32 24 30 37 35 53 67 39 47 23 43 60 61 44 49 36 44\n",
      " 37 63 22 20 58 46 41 41 36 33 21 35 58 51 39 22 46 58 31 53 24 26 26 23\n",
      " 33 39 54 39 47 56 32 32 23 47 47 66 65 41 60 26 56 24 64 64 38 59 46 45\n",
      " 54 26 62 26 21 53 35 38 56 54 46 52 26 37 21 59 32 40 54 45 27 51 56 57\n",
      " 50 52 44 20 53 40 25 51 33 26 35 29 38 62 54 50 34 39 20 44 27 27 31 55\n",
      " 45 37 23 23 49 66 37 48 27 37 44 53 52 45 25 44 25 45 28 26 46 27 27 62\n",
      " 47 40 44 49 26 48 37 33 44 22 25 53 69 34 53 32 38 48 59 43 53 27 60 44\n",
      " 23 40 51 49 43 43 37 60 45 42 42 41 52 28 39 30 50 34 51 52 51 52 32 46\n",
      " 69 37 76 52 66 34 47 45 51 54 47 35 40 45 32 40 43 51 22 42 66 46 60 61\n",
      " 22 50 41 43 47 44 43 26 21 41 26 31 54 44 49 42 34 44 39 39 44 51 61 25\n",
      " 43 69 43 49 52 54 40 60 25 22 48 49 20 49 22 39 49 53 34 27 65 52 47 53\n",
      " 62 36 34 44 61 25 64 39 46 62 54 25 23 26 54 25 50 23 42 22 48 40 51 35\n",
      " 35 24 67 36 28 55 22 55 46 23 57 49 69 57 65 26 62 46 25 39 54 50 25 23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1240/2788383991.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_valid = torch.tensor(X_train, dtype=torch.float32)\n",
      "/tmp/ipykernel_1240/2788383991.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_valid = torch.tensor(y_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALID MAE:3.6544642857142855\n",
      "VALID Variance 97.15196109693878\n",
      "tensor(2.5581)\n",
      "[37, 43, 38, 45, 43, 40, 42, 43, 35, 43, 41, 40, 45, 38, 45, 47, 39, 45, 45, 45, 43, 46, 42, 43, 44, 45, 43, 40, 39, 42, 45, 40, 46, 47, 42, 46, 43, 45, 45, 40, 41, 48, 44, 46, 47, 43, 48, 43, 44, 42, 48, 45, 43, 43, 44, 43, 44, 44, 44, 46, 45, 44, 45, 43, 45, 44, 39, 44, 43, 42, 44, 46, 44, 47, 47, 44, 45, 46, 49, 43, 46, 43, 43, 42, 36, 41, 47, 46, 48, 46, 43, 48, 40, 47, 44, 33, 40, 45, 46, 49, 46, 46, 46, 42, 46, 39, 43, 43, 45, 46, 44, 45, 42, 47, 45, 42, 44, 44, 45, 49, 45, 44, 48, 45, 37, 45, 42, 48, 44, 42, 45, 53, 47, 45, 41, 42, 48, 48, 45, 42, 47, 44, 46, 47, 47, 47, 45, 48, 44, 45, 46, 44, 42, 47, 45, 47, 44, 46, 42, 42, 42, 41, 45, 42, 47, 43, 38, 46, 45, 40, 39, 41, 43, 43, 43, 46, 45, 43, 45, 47, 44, 44, 44, 46, 42, 37, 48, 43, 43, 44, 45, 47, 43, 46, 47, 45, 49, 46, 45, 42, 46, 43, 45, 43, 44, 44, 47, 37, 42, 41, 44, 42, 39, 41, 46, 33, 47, 43, 40, 45, 43, 45, 45, 46, 39, 48, 44, 46, 41, 43, 46, 41, 44, 48, 45, 46, 46, 44, 45, 43, 42, 51, 47, 45, 40, 44, 44, 43, 42, 39, 45, 46, 41, 45, 38, 50, 40, 43, 38, 43, 45, 46, 46, 42, 47, 45, 43, 47, 43, 49, 45, 49, 46, 40, 47, 43, 47, 37, 38, 45, 42, 41, 48, 43, 45, 45, 47, 38, 35, 46, 36, 42, 36, 46, 48, 45, 41, 42, 45, 46, 42, 46, 47, 48, 44, 46, 43, 44, 44, 40, 37, 38, 44, 44, 39, 46, 47, 42, 45, 42, 42, 45, 44, 44, 44, 44, 46, 41, 38, 42, 49, 46, 46, 46, 47, 45, 50, 46, 44, 44, 45, 44, 47, 47, 46, 46, 46, 45, 38, 47, 44, 48, 42, 38, 45, 44, 44, 44, 41, 46, 42, 40, 45, 48, 45, 43, 50, 48, 49, 48, 46, 44, 41, 39, 43, 45, 43, 46, 45, 39, 45, 51, 45, 45, 44, 47, 43, 45, 44]\n",
      "TEST Variance 9.016303090780527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "train_original_x, train_original_y = train_data_preprocess()\n",
    "test_original_x = data_preprocess()\n",
    "\n",
    "# 将数据集分为训练集和测试集\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_original_x, train_original_y, test_size=0.3, random_state=0)\n",
    "print(y_valid)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_valid = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_train, dtype=torch.float32)\n",
    "test_original_x = torch.tensor(test_original_x, dtype=torch.float32)\n",
    "\n",
    "linear_model = RandomForestRegressor(n_estimators=50,random_state=0)\n",
    "linear_model.fit(X_train, y_train)\n",
    "linear_model_valid_prediction = linear_model.predict(X_valid)\n",
    "print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in linear_model_valid_prediction])))\n",
    "print('VALID Variance {}'.format(np.var([int(i) for i in linear_model_valid_prediction])))\n",
    "\n",
    "mae = sum([abs(i - j) for i, j in zip(y_valid, [int(i) for i in linear_model_valid_prediction])]) / 1600\n",
    "print(mae)\n",
    "linear_model_test_prediction = linear_model.predict(test_original_x)\n",
    "print([int(i) for i in linear_model_test_prediction])\n",
    "print('TEST Variance {}'.format(np.var([int(i) for i in linear_model_test_prediction])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
