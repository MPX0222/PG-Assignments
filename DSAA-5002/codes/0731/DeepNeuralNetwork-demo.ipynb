{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "530e8531-083a-40d9-b7b0-f6ff3f4aef4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Cowardly refusing to create an empty archive\n",
      "Try 'tar --help' or 'tar --usage' for more information.\n"
     ]
    }
   ],
   "source": [
    "!tar -zcvf /tmp/test1.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e938914-f15a-4fcd-9487-e9e840fedde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/Test.zip\n",
      "  inflating: Test/aseg - 389.csv     \n",
      "  inflating: Test/lh.GausCurv- 389.csv  \n",
      "  inflating: Test/lh.GrayVol - 389.csv  \n",
      "  inflating: Test/lh.MeanCurv- 389.csv  \n",
      "  inflating: Test/lh.SurfArea - 389.csv  \n",
      "  inflating: Test/lh.ThickAvg- 389.csv  \n",
      "  inflating: Test/rh.GausCurv- 389.csv  \n",
      "  inflating: Test/rh.GrayVol- 389.csv  \n",
      "  inflating: Test/rh.MeanCurv- 389.csv  \n",
      "  inflating: Test/rh.SurfArea - 389.csv  \n",
      "  inflating: Test/rh.ThickAvg- 389.csv  \n",
      "  inflating: Test/subject_info - 389.csv  \n",
      "  inflating: Test/wmparc - 389.csv   \n"
     ]
    }
   ],
   "source": [
    "!unzip data/Test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dde7d56-3012-46b9-b7b0-71f01b76b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from RVM import *\n",
    "\n",
    "\n",
    "# 构建深度网络模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "def data_preprocess():\n",
    "    data_path_1 = r'data/train/lh.MeanCurv - 1600.csv'\n",
    "    data_path_2 = r'data/train/lh.GausCurv - 1600.csv'\n",
    "    data_path_3 = r'data/train/rh.MeanCurv- 1600.csv'\n",
    "    data_path_4 = r'data/train/rh.GausCurv- 1600.csv'\n",
    "\n",
    "    data_path_5 = r'data/train/lh.ThickAvg - 1600.csv'\n",
    "    data_path_6 = r'data/train/lh.SurfArea - 1600.csv'\n",
    "    data_path_7 = r'data/train/rh.ThickAvg- 1600.csv'\n",
    "    data_path_8 = r'data/train/rh.SurfArea - 1600.csv'\n",
    "    \n",
    "    data_path_9 = r'data/train/lh.GrayVol - 1600.csv'\n",
    "    data_path_10 = r'data/train/rh.GrayVol- 1600.csv'\n",
    "    \n",
    "    data_path_11 = r'data/train/wmparc - 1600.csv'\n",
    "    data_path_12 = r'data/train/aseg - 1600.csv'\n",
    "\n",
    "    label_path = r'data/train/subject_info - 1600.csv'\n",
    "\n",
    "    original_x1, original_x2, original_x3, original_x4, original_y = pd.read_csv(data_path_1), pd.read_csv(\n",
    "        data_path_2), pd.read_csv(data_path_3), pd.read_csv(data_path_4), pd.read_csv(label_path)\n",
    "    original_x5, original_x6, original_x7, original_x8 = pd.read_csv(data_path_5), pd.read_csv(\n",
    "        data_path_6), pd.read_csv(data_path_7), pd.read_csv(data_path_8)\n",
    "    original_x9, original_x10 = pd.read_csv(data_path_9), pd.read_csv(data_path_10)\n",
    "    original_x11, original_x12 = pd.read_csv(data_path_11), pd.read_csv(data_path_12)\n",
    "    \n",
    "    original_x1, original_x2, original_x3, original_x4 = original_x1.iloc[:, 1:], original_x2.iloc[:, 1:], original_x3.iloc[:, 1:], original_x4.iloc[:, 1:]\n",
    "    original_x5, original_x6, original_x7, original_x8 = original_x5.iloc[:, 1:], original_x6.iloc[:, 1:], original_x7.iloc[:, 1:], original_x8.iloc[:, 1:]\n",
    "    original_x9, original_x10 = original_x9.iloc[:, 1:], original_x10.iloc[:, 1:]\n",
    "    original_x11, original_x12 = original_x11.iloc[:, 1:], original_x12.iloc[:, 1:]\n",
    "    original_y = original_y.iloc[:, 3]\n",
    "    \n",
    "    # 目前经验是需要标准化    \n",
    "    original_x1 = (original_x1 - original_x1.min()) / (original_x1.max() - original_x1.min())\n",
    "    original_x2 = (original_x2 - original_x2.min()) / (original_x2.max() - original_x2.min())\n",
    "    original_x3 = (original_x3 - original_x3.min()) / (original_x3.max() - original_x3.min())\n",
    "    original_x4 = (original_x4 - original_x4.min()) / (original_x4.max() - original_x4.min())\n",
    "    original_x5 = (original_x5 - original_x5.min()) / (original_x5.max() - original_x5.min())\n",
    "    original_x6 = (original_x6 - original_x6.min()) / (original_x6.max() - original_x6.min())\n",
    "    original_x7 = (original_x7 - original_x7.min()) / (original_x7.max() - original_x7.min())\n",
    "    original_x8 = (original_x8 - original_x8.min()) / (original_x8.max() - original_x8.min())\n",
    "    original_x9 = (original_x9 - original_x9.min()) / (original_x9.max() - original_x9.min())\n",
    "    original_x10 = (original_x10 - original_x10.min()) / (original_x10.max() - original_x10.min())\n",
    "    original_x11 = (original_x11 - original_x11.min()) / (original_x11.max() - original_x11.min())\n",
    "    original_x12 = (original_x12 - original_x12.min()) / (original_x12.max() - original_x12.min())\n",
    "\n",
    "    # # PCA\n",
    "    pca_fitter = PCA(n_components=10)\n",
    "    # original_x1, original_x2, original_x3, original_x4 = pca_fitter.fit_transform(original_x1), pca_fitter.fit_transform(original_x2), pca_fitter.fit_transform(original_x3), pca_fitter.fit_transform(original_x4)\n",
    "    # original_x5, original_x6, original_x7, original_x8 = pca_fitter.fit_transform(original_x5), pca_fitter.fit_transform(original_x6), pca_fitter.fit_transform(original_x7), pca_fitter.fit_transform(original_x8)\n",
    "    # original_x9, original_x10, original_x11, original_x12 = pca_fitter.fit_transform(original_x9), pca_fitter.fit_transform(original_x10), pca_fitter.fit_transform(original_x11), pca_fitter.fit_transform(original_x12)\n",
    "    \n",
    "    #\n",
    "    # # SELECT-K-BEST\n",
    "    # k = 10  # 选择最相关的特征数量\n",
    "    # selector = SelectKBest(f_classif, k=k)\n",
    "    # original_x1, original_x2, original_x3, original_x4 = selector.fit_transform(original_x1, original_y), selector.fit_transform(original_x2, original_y), selector.fit_transform(original_x3, original_y), selector.fit_transform(original_x4, original_y)\n",
    "\n",
    "\n",
    "    original_x1, original_x2, original_x3, original_x4 = np.array(original_x1), np.array(original_x2), np.array(original_x3), np.array(original_x4)\n",
    "    original_x5, original_x6, original_x7, original_x8 = np.array(original_x5), np.array(original_x6), np.array(original_x7), np.array(original_x8)\n",
    "    original_x9, original_x10 = np.array(original_x9), np.array(original_x10)\n",
    "    original_x11, original_x12 = np.array(original_x11), np.array(original_x12)\n",
    "    original_y = np.array(original_y)\n",
    "\n",
    "    # original_x = np.concatenate([original_x1 + original_x3, original_x2 + original_x4], axis=1)\n",
    "    original_x_thick = (original_x5 + original_x7) * 0.5\n",
    "    original_x_gauscurv = (original_x2 + original_x4) * 0.5\n",
    "    original_x_meancurv = (original_x1 + original_x3) * 0.5\n",
    "    original_x_surfarea = (original_x6 + original_x8) * 0.5\n",
    "    original_x_grayvol = (original_x9 + original_x10) * 0.5\n",
    "    original_x_wmparc = original_x11\n",
    "    original_x_aseg = original_x12\n",
    "    \n",
    "    \n",
    "    \n",
    "    original_x = np.hstack([original_x_thick, original_x_gauscurv, original_x_meancurv, original_x_surfarea, original_x_grayvol, original_x_wmparc, original_x_aseg])\n",
    "    # original_x = np.hstack([original_x1, original_x2, original_x3, original_x4, original_x5, original_x6, original_x7, original_x8])\n",
    "    original_x = np.hstack([original_x_thick, original_x_gauscurv, original_x_meancurv, original_x_surfarea, original_x_grayvol, original_x_wmparc, original_x_aseg, \n",
    "                            original_x1, original_x2, original_x3, original_x4, original_x5, original_x6, original_x7, original_x8, original_x9, original_x10, original_x11, original_x12])\n",
    "    \n",
    "\n",
    "    return original_x, original_y\n",
    "\n",
    "\n",
    "def main():\n",
    "    original_x, original_y = data_preprocess()\n",
    "    print(original_x.shape, original_y.shape)\n",
    "\n",
    "    # 将数据集分为训练集和测试集\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(original_x, original_y, test_size=0.2, random_state=0)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_valid = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_valid = torch.tensor(y_train, dtype=torch.float32)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "    # 定义输入数据的维度\n",
    "    input_dim = X_train.shape[1]\n",
    "    total_epoch = 300\n",
    "    model = Net(input_dim)\n",
    "    criterion = nn.L1Loss()  # MAE Loss\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.90)  # 使用Adam优化器进行参数更新\n",
    "\n",
    "    # 对训练数据进行拟合\n",
    "    for epoch in range(total_epoch):  # 迭代次数为100轮\n",
    "        train_loss = 0\n",
    "        for i, (inputs, labels) in enumerate(train_dataloader):  # 遍历每个batch的数据和标签\n",
    "            optimizer.zero_grad()  # 清空梯度缓存区以便进行反向传播计算梯度。同时需要注意的是，在实际应用中需要对训练数据进行相同的预处理操作。\n",
    "            outputs = model(inputs)  # 通过模型进行预测得到输出结果\n",
    "            loss = criterion(outputs, labels)  # 计算预测结果与真实结果之间的均方误差损失函数值。同时需要注意的是，在实际应用中需要对训练数据进行相同的预处理操作。\n",
    "            loss.backward()  # 对损失函数进行反向传播计算梯度。同时需要注意的是，在实际应用中需要对训练数据进行相同的预处理操作。\n",
    "            optimizer.step()  # 根据梯度下降算法更新参数。同时需要注意的是，在实际应用中需要对训练数据进行相同的预处理操作。\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # if epoch > 15:\n",
    "        #     print(outputs)\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}'.format(epoch + 1, total_epoch, train_loss / (i + 1)))  # 这里采用了print函数来打印损失函数值。同时需要注意的是，在实际应用中需要对训练数据进行相同的预处理操作。\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0\n",
    "            for i, (valid_inputs, valid_labels) in enumerate(valid_dataloader):\n",
    "                valid_outputs = model(valid_inputs)\n",
    "                valid_loss_item = criterion(valid_outputs, valid_labels)\n",
    "                valid_loss += valid_loss_item\n",
    "\n",
    "            print('Epoch [{}/{}], Valid Loss: {:.4f}'.format(epoch + 1, total_epoch, valid_loss / (i + 1)))\n",
    "\n",
    "        # 保存模型\n",
    "        torch.save(model, '5_fc_layer_model-ensemble_thick_gaus_mean_surf.pkl')\n",
    "\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85a98757-9c41-455c-bd53-d446436b68de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 659) (1600,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_861/808523506.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_valid = torch.tensor(X_train, dtype=torch.float32)\n",
      "/tmp/ipykernel_861/808523506.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_valid = torch.tensor(y_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39.18023841 44.76743257 42.25584554 ... 58.21920266 61.2432337\n",
      " 27.50917941]\n",
      "MAE:8.054679137116066\n",
      "MAE:8.311914207031\n",
      "MAE:8.388002814715515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def rvm_main():\n",
    "    original_x, original_y = data_preprocess()\n",
    "    print(original_x.shape, original_y.shape)\n",
    "\n",
    "    # 将数据集分为训练集和测试集\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(original_x, original_y, test_size=0.3, random_state=0)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_valid = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_valid = torch.tensor(y_train, dtype=torch.float32)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=1024, shuffle=True)\n",
    "    \n",
    "    model = RVR(kernel=\"linear\")\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_valid)\n",
    "    print(prediction)\n",
    "    print('MAE:{}'.format(mean_absolute_error(y_valid, prediction)))\n",
    "\n",
    "    model = RVR(kernel=\"rbf\")\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_valid)\n",
    "    print('MAE:{}'.format(mean_absolute_error(y_valid, prediction)))\n",
    "\n",
    "\n",
    "    model = RVR(kernel=\"poly\")\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_valid)\n",
    "    print('MAE:{}'.format(mean_absolute_error(y_valid, prediction)))\n",
    "\n",
    "    \n",
    "rvm_main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
