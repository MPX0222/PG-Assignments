{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***2023 Brain Age Predition - iFLYTEK A.I. Developer Competition***\n",
    "\n",
    "*Background: 我国已进入人口老龄化社会，这将是我国本世纪面临的一项重要挑战。异常老化引起的各种疾病（如阿尔茨海默病、帕金森病）已带来了日益严重的经济和社会问题。基于结构磁共振的大脑年龄被广泛应用于刻画大脑的老化过程，预测脑龄和实际生理年龄的差值（Predicted Age Difference，PAD），即偏离正常大脑老化轨迹的程度，可作为衡量个体异常老化的客观指标。研究表明，多种类型的神经系统疾病、代谢性疾病等都与大脑异常老化相关。老人的PAD值越大，则其出现神经精神问题的风险就越高。脑龄预测为探索大脑在老化过程中的异常变化及神经精神疾病如何影响正常老化提供了一种新方法，为研究大脑老化的个体差异提供了新视角。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***1 Load Dataset***\n",
    "\n",
    "***This Dataset contains 12 csv files, which come from MRI scanner.***\n",
    "\n",
    "***This part aims to load the dataset and use pre-process methods to construct data features***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_1 = r'data/train/lh.MeanCurv - 1600.csv'\n",
    "data_path_2 = r'data/train/lh.GausCurv - 1600.csv'\n",
    "data_path_3 = r'data/train/rh.MeanCurv- 1600.csv'\n",
    "data_path_4 = r'data/train/rh.GausCurv- 1600.csv'\n",
    "\n",
    "data_path_5 = r'data/train/lh.ThickAvg - 1600.csv'\n",
    "data_path_6 = r'data/train/lh.SurfArea - 1600.csv'\n",
    "data_path_7 = r'data/train/rh.ThickAvg- 1600.csv'\n",
    "data_path_8 = r'data/train/rh.SurfArea - 1600.csv'\n",
    "\n",
    "data_path_9 = r'data/train/lh.GrayVol - 1600.csv'\n",
    "data_path_10 = r'data/train/rh.GrayVol- 1600.csv'\n",
    "\n",
    "data_path_11 = r'data/train/wmparc - 1600.csv'\n",
    "data_path_12 = r'data/train/aseg - 1600.csv'\n",
    "\n",
    "label_path = r'data/train/subject_info - 1600.csv'\n",
    "\n",
    "original_x1, original_x2, original_x3, original_x4, original_y = pd.read_csv(data_path_1), pd.read_csv(data_path_2), pd.read_csv(data_path_3), pd.read_csv(data_path_4), pd.read_csv(label_path)\n",
    "original_x5, original_x6, original_x7, original_x8 = pd.read_csv(data_path_5), pd.read_csv(data_path_6), pd.read_csv(data_path_7), pd.read_csv(data_path_8)\n",
    "original_x9, original_x10 = pd.read_csv(data_path_9), pd.read_csv(data_path_10)\n",
    "original_x11, original_x12 = pd.read_csv(data_path_11), pd.read_csv(data_path_12)\n",
    "\n",
    "# drop the label numbers\n",
    "original_x1, original_x2, original_x3, original_x4 = original_x1.iloc[:, 1:], original_x2.iloc[:,1:], original_x3.iloc[:,1:], original_x4.iloc[:, 1:]\n",
    "original_x5, original_x6, original_x7, original_x8 = original_x5.iloc[:, 1:], original_x6.iloc[:,1:], original_x7.iloc[:,1:], original_x8.iloc[:, 1:]\n",
    "original_x9, original_x10 = original_x9.iloc[:, 1:], original_x10.iloc[:, 1:]\n",
    "original_x11, original_x12 = original_x11.iloc[:, 1:], original_x12.iloc[:, 1:]\n",
    "original_y = original_y.iloc[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use missingno to check the dataframe\n",
    "msno.matrix(original_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RVM import *\n",
    "\n",
    "\n",
    "def train_data_preprocess():\n",
    "    data_path_1 = r'data/train/lh.MeanCurv - 1600.csv'\n",
    "    data_path_2 = r'data/train/lh.GausCurv - 1600.csv'\n",
    "    data_path_3 = r'data/train/rh.MeanCurv- 1600.csv'\n",
    "    data_path_4 = r'data/train/rh.GausCurv- 1600.csv'\n",
    "\n",
    "    data_path_5 = r'data/train/lh.ThickAvg - 1600.csv'\n",
    "    data_path_6 = r'data/train/lh.SurfArea - 1600.csv'\n",
    "    data_path_7 = r'data/train/rh.ThickAvg- 1600.csv'\n",
    "    data_path_8 = r'data/train/rh.SurfArea - 1600.csv'\n",
    "\n",
    "    data_path_9 = r'data/train/lh.GrayVol - 1600.csv'\n",
    "    data_path_10 = r'data/train/rh.GrayVol- 1600.csv'\n",
    "\n",
    "    data_path_11 = r'data/train/wmparc - 1600.csv'\n",
    "    data_path_12 = r'data/train/aseg - 1600.csv'\n",
    "\n",
    "    label_path = r'data/train/subject_info - 1600.csv'\n",
    "\n",
    "    original_x1, original_x2, original_x3, original_x4, original_y = pd.read_csv(data_path_1), pd.read_csv(data_path_2), pd.read_csv(data_path_3), pd.read_csv(data_path_4), pd.read_csv(label_path)\n",
    "    original_x5, original_x6, original_x7, original_x8 = pd.read_csv(data_path_5), pd.read_csv(data_path_6), pd.read_csv(data_path_7), pd.read_csv(data_path_8)\n",
    "    original_x9, original_x10 = pd.read_csv(data_path_9), pd.read_csv(data_path_10)\n",
    "    original_x11, original_x12 = pd.read_csv(data_path_11), pd.read_csv(data_path_12)\n",
    "\n",
    "    original_x1, original_x2, original_x3, original_x4 = original_x1.iloc[:, 1:], original_x2.iloc[:,1:], original_x3.iloc[:,1:], original_x4.iloc[:, 1:]\n",
    "    original_x5, original_x6, original_x7, original_x8 = original_x5.iloc[:, 1:], original_x6.iloc[:,1:], original_x7.iloc[:,1:], original_x8.iloc[:, 1:]\n",
    "    original_x9, original_x10 = original_x9.iloc[:, 1:], original_x10.iloc[:, 1:]\n",
    "    original_x11, original_x12 = original_x11.iloc[:, 1:], original_x12.iloc[:, 1:]\n",
    "    original_y = original_y.iloc[:, 3]\n",
    "\n",
    "    # 目前经验是需要标准化\n",
    "    original_x1 = (original_x1 - original_x1.min()) / (original_x1.max() - original_x1.min())\n",
    "    original_x2 = (original_x2 - original_x2.min()) / (original_x2.max() - original_x2.min())\n",
    "    original_x3 = (original_x3 - original_x3.min()) / (original_x3.max() - original_x3.min())\n",
    "    original_x4 = (original_x4 - original_x4.min()) / (original_x4.max() - original_x4.min())\n",
    "    original_x5 = (original_x5 - original_x5.min()) / (original_x5.max() - original_x5.min())\n",
    "    original_x6 = (original_x6 - original_x6.min()) / (original_x6.max() - original_x6.min())\n",
    "    original_x7 = (original_x7 - original_x7.min()) / (original_x7.max() - original_x7.min())\n",
    "    original_x8 = (original_x8 - original_x8.min()) / (original_x8.max() - original_x8.min())\n",
    "    original_x9 = (original_x9 - original_x9.min()) / (original_x9.max() - original_x9.min())\n",
    "    original_x10 = (original_x10 - original_x10.min()) / (original_x10.max() - original_x10.min())\n",
    "    original_x11 = (original_x11 - original_x11.min()) / (original_x11.max() - original_x11.min())\n",
    "    original_x12 = (original_x12 - original_x12.min()) / (original_x12.max() - original_x12.min())\n",
    "\n",
    "\n",
    "    original_x1, original_x2, original_x3, original_x4 = np.array(original_x1), np.array(original_x2), np.array(\n",
    "        original_x3), np.array(original_x4)\n",
    "    original_x5, original_x6, original_x7, original_x8 = np.array(original_x5), np.array(original_x6), np.array(\n",
    "        original_x7), np.array(original_x8)\n",
    "    original_x9, original_x10 = np.array(original_x9), np.array(original_x10)\n",
    "    original_x11, original_x12 = np.array(original_x11), np.array(original_x12)\n",
    "    original_y = np.array(original_y)\n",
    "\n",
    "    original_x_thick = (original_x5 + original_x7) * 0.5\n",
    "    original_x_gauscurv = (original_x2 + original_x4) * 0.5\n",
    "    original_x_meancurv = (original_x1 + original_x3) * 0.5\n",
    "    original_x_surfarea = (original_x6 + original_x8) * 0.5\n",
    "    original_x_grayvol = (original_x9 + original_x10) * 0.5\n",
    "    original_x_wmparc = original_x11\n",
    "    original_x_aseg = original_x12\n",
    "\n",
    "    # original_x = np.hstack([original_x_thick, original_x_gauscurv, original_x_meancurv, original_x_surfarea, original_x_grayvol, original_x_wmparc, original_x_aseg])\n",
    "    # original_x = np.hstack([original_x1, original_x2, original_x3, original_x4, original_x5, original_x6, original_x7, original_x8])\n",
    "    original_x = np.hstack(\n",
    "        [original_x_thick, original_x_gauscurv, original_x_meancurv, original_x_surfarea, original_x_grayvol,\n",
    "         original_x_wmparc, original_x_aseg, original_x1, original_x2, original_x3, original_x4, original_x5,\n",
    "         original_x6, original_x7, original_x8, original_x9, original_x10])\n",
    "\n",
    "    return original_x, original_y\n",
    "\n",
    "\n",
    "def test_data_preprocess():\n",
    "    data_path_1 = r'data/test/lh.MeanCurv- 389.csv'\n",
    "    data_path_2 = r'data/test/lh.GausCurv- 389.csv'\n",
    "    data_path_3 = r'data/test/rh.MeanCurv- 389.csv'\n",
    "    data_path_4 = r'data/test/rh.GausCurv- 389.csv'\n",
    "\n",
    "    data_path_5 = r'data/test/lh.ThickAvg- 389.csv'\n",
    "    data_path_6 = r'data/test/lh.SurfArea - 389.csv'\n",
    "    data_path_7 = r'data/test/rh.ThickAvg- 389.csv'\n",
    "    data_path_8 = r'data/test/rh.SurfArea - 389.csv'\n",
    "\n",
    "    data_path_9 = r'data/test/lh.GrayVol - 389.csv'\n",
    "    data_path_10 = r'data/test/rh.GrayVol- 389.csv'\n",
    "\n",
    "    data_path_11 = r'data/test/wmparc - 389.csv'\n",
    "    data_path_12 = r'data/test/aseg - 389.csv'\n",
    "\n",
    "    # label_path = r'data/test/subject_info - 389.csv'\n",
    "\n",
    "    original_x1, original_x2, original_x3, original_x4 = pd.read_csv(data_path_1), pd.read_csv(\n",
    "        data_path_2).dropna(), pd.read_csv(data_path_3), pd.read_csv(data_path_4).dropna()\n",
    "    original_x5, original_x6, original_x7, original_x8 = pd.read_csv(data_path_5), pd.read_csv(\n",
    "        data_path_6), pd.read_csv(data_path_7), pd.read_csv(data_path_8)\n",
    "    original_x9, original_x10 = pd.read_csv(data_path_9), pd.read_csv(data_path_10)\n",
    "    original_x11, original_x12 = pd.read_csv(data_path_11), pd.read_csv(data_path_12)\n",
    "\n",
    "    original_x1, original_x2, original_x3, original_x4 = original_x1.iloc[:, 1:], original_x2.iloc[:,1:], original_x3.iloc[:,1:], original_x4.iloc[:, 1:]\n",
    "    original_x5, original_x6, original_x7, original_x8 = original_x5.iloc[:, 1:], original_x6.iloc[:,1:], original_x7.iloc[:,1:], original_x8.iloc[:, 1:]\n",
    "    original_x9, original_x10 = original_x9.iloc[:, 1:], original_x10.iloc[:, 1:]\n",
    "    original_x11, original_x12 = original_x11.iloc[:, 1:], original_x12.iloc[:, 1:]\n",
    "    # original_y = original_y.iloc[:, 3]\n",
    "\n",
    "    # 目前经验是需要标准化\n",
    "    original_x1 = (original_x1 - original_x1.min()) / (original_x1.max() - original_x1.min())\n",
    "    original_x2 = (original_x2 - original_x2.min()) / (original_x2.max() - original_x2.min())\n",
    "    original_x3 = (original_x3 - original_x3.min()) / (original_x3.max() - original_x3.min())\n",
    "    original_x4 = (original_x4 - original_x4.min()) / (original_x4.max() - original_x4.min())\n",
    "    original_x5 = (original_x5 - original_x5.min()) / (original_x5.max() - original_x5.min())\n",
    "    original_x6 = (original_x6 - original_x6.min()) / (original_x6.max() - original_x6.min())\n",
    "    original_x7 = (original_x7 - original_x7.min()) / (original_x7.max() - original_x7.min())\n",
    "    original_x8 = (original_x8 - original_x8.min()) / (original_x8.max() - original_x8.min())\n",
    "    original_x9 = (original_x9 - original_x9.min()) / (original_x9.max() - original_x9.min())\n",
    "    original_x10 = (original_x10 - original_x10.min()) / (original_x10.max() - original_x10.min())\n",
    "    original_x11 = (original_x11 - original_x11.min()) / (original_x11.max() - original_x11.min())\n",
    "    original_x12 = (original_x12 - original_x12.min()) / (original_x12.max() - original_x12.min())\n",
    "\n",
    "    original_x1, original_x2, original_x3, original_x4 = np.array(original_x1), np.array(original_x2), np.array(\n",
    "        original_x3), np.array(original_x4)\n",
    "    original_x5, original_x6, original_x7, original_x8 = np.array(original_x5), np.array(original_x6), np.array(\n",
    "        original_x7), np.array(original_x8)\n",
    "    original_x9, original_x10 = np.array(original_x9), np.array(original_x10)\n",
    "    original_x11, original_x12 = np.array(original_x11), np.array(original_x12)\n",
    "\n",
    "    # Linear Combination of features\n",
    "    original_x_thick = (original_x5 + original_x7) * 0.5\n",
    "    original_x_gauscurv = (original_x2 + original_x4) * 0.5\n",
    "    original_x_meancurv = (original_x1 + original_x3) * 0.5\n",
    "    original_x_surfarea = (original_x6 + original_x8) * 0.5\n",
    "    original_x_grayvol = (original_x9 + original_x10) * 0.5\n",
    "    original_x_wmparc = original_x11\n",
    "    original_x_aseg = original_x12\n",
    "\n",
    "    # Feature Blending\n",
    "    original_x = np.hstack(\n",
    "        [original_x_thick, original_x_gauscurv, original_x_meancurv, original_x_surfarea, original_x_grayvol,\n",
    "         original_x_wmparc, original_x_aseg, original_x1, original_x2, original_x3, original_x4, original_x5,\n",
    "         original_x6, original_x7, original_x8, original_x9, original_x10])\n",
    "\n",
    "    return original_x\n",
    "\n",
    "\n",
    "def write_submission(prediction_list, submit_name):\n",
    "    ori_submission_path = r'subject_info - 389.csv'\n",
    "\n",
    "    predict_df = pd.read_csv(ori_submission_path, encoding='gbk')\n",
    "    predict_df['年龄'] = prediction_list\n",
    "\n",
    "    print('\\nSumission CSV Preview')\n",
    "    print(predict_df.head(10))\n",
    "    predict_df.to_csv(submit_name + '.csv')\n",
    "\n",
    "    print('Successfully Build a Submission CSV File')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***2 Models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from RVM import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original_x, train_original_y = train_data_preprocess()\n",
    "test_original_x = test_data_preprocess()\n",
    "\n",
    "# 将数据集分为训练集和测试集\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_original_x, train_original_y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_valid = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_train, dtype=torch.float32)\n",
    "test_original_x = torch.tensor(test_original_x, dtype=torch.float32)\n",
    "\n",
    "# RVR\n",
    "print('\\n RVR Model')\n",
    "\n",
    "linear_model = RVR(kernel=\"linear\")\n",
    "linear_model.fit(X_train, y_train)\n",
    "linear_model_valid_prediction = linear_model.predict(X_valid)\n",
    "print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in linear_model_valid_prediction])))\n",
    "print('VALID Variance {}'.format(np.var([int(i) for i in linear_model_valid_prediction])))\n",
    "linear_model_test_prediction = linear_model.predict(test_original_x)\n",
    "print([int(i) for i in linear_model_test_prediction])\n",
    "print('TEST Variance {}'.format(np.var([int(i) for i in linear_model_test_prediction])))\n",
    "\n",
    "rbf_model = RVR(kernel=\"rbf\")\n",
    "rbf_model.fit(X_train, y_train)\n",
    "rbf_model_valid_prediction = rbf_model.predict(X_valid)\n",
    "print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in rbf_model_valid_prediction])))\n",
    "print('VALID Variance {}'.format(np.var([int(i) for i in rbf_model_valid_prediction])))\n",
    "rbf_model_test_prediction = rbf_model.predict(test_original_x)\n",
    "print([int(i) for i in rbf_model_test_prediction])\n",
    "print('TEST Variance {}'.format(np.var([int(i) for i in rbf_model_test_prediction])))\n",
    "\n",
    "poly_model = RVR(kernel=\"poly\")\n",
    "poly_model.fit(X_train, y_train)\n",
    "poly_model_valid_prediction = poly_model.predict(X_valid)\n",
    "print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in poly_model_valid_prediction])))\n",
    "print('VALID Variance {}'.format(np.var([int(i) for i in poly_model_valid_prediction])))\n",
    "poly_model_test_prediction = rbf_model.predict(test_original_x)\n",
    "print([int(i) for i in poly_model_test_prediction])\n",
    "print('TEST Variance {}'.format(np.var([int(i) for i in poly_model_test_prediction])))\n",
    "\n",
    "# 随机森林\n",
    "print('\\n RandomForest Model')\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=12, random_state=0)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_model_valid_prediction = rf_model.predict(X_valid)\n",
    "print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in rf_model_valid_prediction])))\n",
    "print('VALID Variance {}'.format(np.var([int(i) for i in rf_model_valid_prediction])))\n",
    "rf_model_test_prediction = rf_model.predict(test_original_x)\n",
    "print([int(i) for i in rf_model_test_prediction])\n",
    "print('TEST Variance {}'.format(np.var([int(i) for i in rf_model_test_prediction])))\n",
    "\n",
    "# Bagging\n",
    "print('\\n Bagging Model')\n",
    "\n",
    "bag_model = BaggingRegressor()\n",
    "bag_model.fit(X_train, y_train)\n",
    "bag_model_valid_prediction = bag_model.predict(X_valid)\n",
    "print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in bag_model_valid_prediction])))\n",
    "print('VALID Variance {}'.format(np.var([int(i) for i in bag_model_valid_prediction])))\n",
    "bag_model_test_prediction = bag_model.predict(test_original_x)\n",
    "print([int(i) for i in bag_model_test_prediction])\n",
    "print('TEST Variance {}'.format(np.var([int(i) for i in bag_model_test_prediction])))\n",
    "\n",
    "# XGBoost\n",
    "print('\\n XGBoost Model')\n",
    "\n",
    "xgb_model = XGBRegressor(learning_rate=0.2)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_model_valid_prediction = xgb_model.predict(X_valid)\n",
    "print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in xgb_model_valid_prediction])))\n",
    "print('LABEL Variance {}'.format(np.var([int(i) for i in y_valid])))\n",
    "print('VALID Variance {}'.format(np.var([int(i) for i in xgb_model_valid_prediction])))\n",
    "xgb_model_test_prediction = xgb_model.predict(test_original_x)\n",
    "print([int(i) for i in xgb_model_test_prediction])\n",
    "print('TEST Variance {}'.format(np.var([int(i) for i in xgb_model_test_prediction])))\n",
    "\n",
    "# LightGBM\n",
    "print('\\n LightGBM Model')\n",
    "\n",
    "lgbm_model = LGBMRegressor()\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "lgbm_model_valid_prediction = lgbm_model.predict(X_valid)\n",
    "print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in lgbm_model_valid_prediction])))\n",
    "print('LABEL Variance {}'.format(np.var([int(i) for i in y_valid])))\n",
    "print('VALID Variance {}'.format(np.var([int(i) for i in lgbm_model_valid_prediction])))\n",
    "lgbm_model_test_prediction = lgbm_model.predict(test_original_x)\n",
    "print([int(i) for i in lgbm_model_test_prediction])\n",
    "print('TEST Variance {}'.format(np.var([int(i) for i in lgbm_model_test_prediction])))\n",
    "\n",
    "# 集成模型\n",
    "print('\\n Ensemble Model')\n",
    "\n",
    "new_linear_pred, new_rbf_pred, new_poly_pred = [int(i) for i in linear_model_valid_prediction], [int(i) for i in rbf_model_valid_prediction], [int(i) for i in poly_model_valid_prediction]\n",
    "new_rf_pred = [int(i) for i in rf_model_valid_prediction]\n",
    "new_bag_pred = [int(i) for i in bag_model_valid_prediction]\n",
    "new_xgb_pred = [int(i) for i in xgb_model_valid_prediction]\n",
    "new_lgbm_pred = [int(i) for i in lgbm_model_valid_prediction]\n",
    "\n",
    "# ensemble_prediction = [(i + j) / 2 for i, j in zip(new_rf_pred, new_rbf_pred)]\n",
    "ensemble_prediction = [(i + j + k) / 3 for i, j, k in zip(new_xgb_pred, new_bag_pred, new_linear_pred)]\n",
    "print('\\nENSEMBLE VALID MAE:{}'.format(mean_absolute_error(y_valid, ensemble_prediction)))\n",
    "print('ENSEMBLE VALID Variance {}'.format(np.var([int(i) for i in ensemble_prediction])))\n",
    "\n",
    "test_new_linear_pred, test_new_rbf_pred, test_new_poly_pred = [int(i) for i in linear_model_test_prediction], [\n",
    "    int(i) for i in rbf_model_test_prediction], [int(i) for i in poly_model_test_prediction]\n",
    "test_new_bag_pred = [int(i) for i in bag_model_test_prediction]\n",
    "test_new_rf_pred = [int(i) for i in rf_model_test_prediction]\n",
    "test_new_xgb_pref = [int(i) for i in xgb_model_test_prediction]\n",
    "\n",
    "# test_ensemble_prediction = [(i + j) / 2 for i, j in zip(test_new_rf_pred, test_new_rbf_pred)]\n",
    "test_ensemble_prediction = [(i + j + k) / 3 for i, j, k in\n",
    "                            zip(test_new_xgb_pref, test_new_bag_pred, test_new_linear_pred)]\n",
    "print([int(i) for i in test_ensemble_prediction])\n",
    "print('TEST Variance {}'.format(np.var([int(i) for i in test_ensemble_prediction])))\n",
    "\n",
    "# 写结果\n",
    "# final_list = [int(i) for i in [int(i) for i in lgbm_model_test_prediction]]\n",
    "# submission_version_name = 'Ensemble-DataAug3-XGBoost_Bagging_RVR'\n",
    "# write_submission(final_list, submission_version_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
