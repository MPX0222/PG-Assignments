{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***2023 Brain Age Predition - iFLYTEK A.I. Developer Competition***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***1 Load Dataset***\n",
    "\n",
    "***This Dataset contains 12 csv files, which come from MRI scanner.***\n",
    "\n",
    "***This part aims to load the dataset and use pre-process methods to construct data features***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "\n",
    "train_datafolder_path = 'data/train'\n",
    "test_datafolder_path = 'data/test'\n",
    "\n",
    "def read_datafile(datafolder_path, is_test=False):\n",
    "    feature_dict = {}\n",
    "    for file_name in os.listdir(datafolder_path):\n",
    "        data_path = os.path.join(datafolder_path, file_name)\n",
    "        feature_df = pd.read_csv(data_path)\n",
    "        feature_name = file_name[:file_name.find('-')]\n",
    "        print(feature_name)\n",
    "\n",
    "        if 'subject_info' in data_path:\n",
    "            if not is_test:\n",
    "                # get label colunm from y_dataframe\n",
    "                label_df = feature_df.iloc[:, 3]\n",
    "                feature_dict[feature_name] = label_df\n",
    "            else:\n",
    "                feature_dict[feature_name] = None\n",
    "        else:\n",
    "            # drop the label numbers of x_dataframe\n",
    "            feature_df = feature_df.iloc[:, 1:].dropna()\n",
    "            feature_dict[feature_name] = feature_df\n",
    "\n",
    "    return feature_dict\n",
    "\n",
    "train_feature_dict, test_feature_dict = read_datafile(train_datafolder_path), read_datafile(test_datafolder_path, is_test=True)\n",
    "\n",
    "print(train_feature_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use missingno to check the dataframe\n",
    "for index, name in enumerate(train_feature_dict.keys()):\n",
    "    if name != 'subject_info':\n",
    "        msno.matrix(train_feature_dict[name], figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# use seaborm to visualize the distribution\n",
    "for index, name in enumerate(train_feature_dict.keys()):\n",
    "    if name != 'subject_info':\n",
    "        sns.distplot(train_feature_dict[name], bins=100, color=\"green\") \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TruncatedPowerBasisFunctionExpanding(X, knots):\n",
    "    X_dk = X\n",
    "    k_j = len(knots)\n",
    "    for h in range(k_j-3):\n",
    "        X_dk = np.column_stack([X_dk, basis_fuction(X, knots, h) - basis_fuction(X, knots, k_j-2)])\n",
    "    return X_dk\n",
    "\n",
    "def basis_fuction(X, knots, h):\n",
    "    a = np.power(np.abs(X-knots[h]), 3)\n",
    "    b = np.power(np.abs(X-knots[-1]), 3)\n",
    "    return ((a - b) / (knots[-1] - knots[h]))\n",
    "\n",
    "def data_preprocess(data_dict):\n",
    "    # Min-Max\n",
    "    new_data_dict = {}\n",
    "    for index, feature_df in enumerate(data_dict.keys()):\n",
    "        if 'subject_info' not in feature_df:\n",
    "            new_data_dict[feature_df] = (data_dict[feature_df] - data_dict[feature_df].min()) / (data_dict[feature_df].max() - data_dict[feature_df].min())\n",
    "            # new_data_dict[feature_df] = np.array(data_dict[feature_df])\n",
    "        else:\n",
    "            new_data_dict[feature_df] = data_dict[feature_df] \n",
    "    return new_data_dict\n",
    "\n",
    "def feature_aug(data_dict):\n",
    "    new_data_dict = {}\n",
    "    for index, feature_df in enumerate(data_dict.keys()):\n",
    "        if 'subject_info' not in feature_df:\n",
    "            new_data_dict[feature_df] = np.array(data_dict[feature_df], dtype=np.float32)\n",
    "            new_data_dict[feature_df] = TruncatedPowerBasisFunctionExpanding(data_dict[feature_df], [0, 0.3, 0.6, 0.9, 1])\n",
    "        else:\n",
    "            new_data_dict[feature_df] = data_dict[feature_df] \n",
    "    return new_data_dict\n",
    "\n",
    "def feature_combination(data_dict, is_test=False):\n",
    "\n",
    "    original_x_thick = (data_dict['lh.ThickAvg'] + data_dict['rh.ThickAvg']) * 0.5\n",
    "    original_x_gauscurv = (data_dict['lh.GausCurv'] + data_dict['rh.GausCurv']) * 0.5\n",
    "    original_x_meancurv = (data_dict['lh.MeanCurv'] + data_dict['rh.MeanCurv']) * 0.5\n",
    "    original_x_surfarea = (data_dict['lh.SurfArea'] + data_dict['rh.SurfArea']) * 0.5\n",
    "    original_x_grayvol = (data_dict['lh.GrayVol'] + data_dict['rh.GrayVol']) * 0.5\n",
    "    original_x_wmparc = data_dict['wmparc']\n",
    "    original_x_aseg = data_dict['aseg']\n",
    "\n",
    "    original_x = np.hstack([\n",
    "            # original_x_thick, original_x_gauscurv, original_x_meancurv, original_x_surfarea, original_x_grayvol, original_x_wmparc, original_x_aseg, \n",
    "            data_dict['lh.ThickAvg'], data_dict['rh.ThickAvg'], data_dict['lh.GausCurv'], data_dict['rh.GausCurv'], data_dict['lh.MeanCurv'],\n",
    "            data_dict['rh.MeanCurv'], data_dict['lh.SurfArea'], data_dict['rh.SurfArea'], data_dict['lh.GrayVol'], data_dict['rh.GrayVol']\n",
    "        ])\n",
    "    \n",
    "    if not is_test:\n",
    "        original_y = data_dict['subject_info']\n",
    "        return original_x, original_y\n",
    "    else:\n",
    "        return original_x\n",
    "\n",
    "# train_x, train_y = feature_combination(data_preprocess(feature_aug(train_feature_dict)))\n",
    "train_x, train_y = feature_combination(data_preprocess(train_feature_dict))\n",
    "# test_x = feature_combination(data_preprocess(feature_aug(test_feature_dict)), is_test=True)\n",
    "test_x = feature_combination(data_preprocess(test_feature_dict), is_test=True)\n",
    "print(train_x)\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***2 Models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from RVM import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_671791/1904674775.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_x = torch.tensor(test_x, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1280, 310])\n",
      "(1280,)\n",
      "torch.Size([389, 310])\n",
      "\n",
      " RVR Model\n",
      "\n",
      "VALID MAE:10.2\n",
      "VALID Variance 60.861093749999995\n",
      "LABEL Variance 195.26875\n",
      "[29, 33, 26, 40, 35, 35, 35, 33, 29, 39, 33, 25, 34, 18, 40, 37, 32, 37, 35, 28, 32, 42, 44, 32, 34, 37, 28, 27, 32, 36, 32, 38, 41, 41, 27, 51, 43, 46, 28, 28, 29, 35, 39, 43, 38, 40, 42, 42, 51, 40, 47, 52, 34, 38, 44, 33, 33, 44, 36, 40, 44, 44, 39, 25, 46, 42, 16, 31, 40, 31, 45, 50, 28, 49, 53, 38, 46, 42, 36, 38, 40, 42, 45, 50, 22, 38, 38, 52, 49, 39, 41, 49, 30, 47, 36, 17, 42, 41, 41, 56, 34, 57, 49, 38, 38, 23, 38, 31, 31, 35, 38, 40, 37, 45, 37, 34, 39, 44, 40, 47, 37, 26, 37, 45, 30, 38, 37, 40, 46, 38, 44, 47, 52, 43, 41, 34, 51, 51, 43, 45, 43, 39, 49, 57, 45, 44, 38, 32, 39, 38, 34, 36, 25, 39, 40, 45, 36, 36, 18, 26, 29, 34, 44, 29, 44, 40, 25, 32, 42, 29, 23, 23, 27, 39, 36, 49, 43, 27, 41, 40, 40, 26, 33, 34, 26, 19, 51, 42, 34, 56, 51, 40, 41, 50, 46, 35, 43, 24, 22, 30, 40, 35, 19, 40, 44, 32, 36, 33, 37, 28, 47, 25, 14, 31, 31, 28, 40, 33, 34, 35, 31, 43, 39, 53, 37, 42, 51, 41, 46, 42, 40, 41, 49, 40, 42, 43, 45, 24, 34, 35, 42, 47, 45, 40, 25, 24, 29, 35, 34, 31, 41, 35, 19, 36, 19, 55, 28, 36, 42, 57, 37, 39, 45, 46, 53, 50, 37, 39, 41, 53, 41, 63, 41, 36, 50, 33, 44, 44, 36, 36, 31, 43, 53, 29, 33, 40, 43, 27, 21, 38, 12, 39, 11, 52, 43, 41, 32, 45, 45, 39, 23, 40, 34, 47, 55, 56, 32, 29, 46, 42, 15, 26, 36, 34, 27, 37, 36, 36, 42, 34, 30, 38, 31, 38, 37, 36, 44, 33, 25, 32, 49, 43, 49, 48, 43, 39, 43, 49, 42, 46, 40, 38, 43, 47, 45, 33, 58, 42, 34, 49, 44, 43, 35, 35, 46, 45, 49, 43, 38, 38, 31, 39, 47, 56, 49, 48, 53, 46, 52, 44, 51, 40, 35, 41, 42, 40, 36, 38, 39, 25, 33, 47, 26, 37, 35, 36, 34, 30, 37]\n",
      "TEST Variance 73.56497776250488\n",
      "\n",
      " XGBoost Model\n",
      "\n",
      "VALID MAE:9.759375\n",
      "LABEL Variance 195.26875\n",
      "VALID Variance 53.698037109374994\n",
      "[41, 44, 33, 48, 46, 43, 39, 50, 30, 49, 40, 44, 48, 36, 43, 41, 40, 44, 42, 44, 42, 50, 52, 44, 47, 42, 47, 40, 33, 49, 45, 48, 50, 49, 45, 45, 39, 42, 40, 44, 42, 47, 48, 50, 45, 49, 47, 53, 50, 49, 48, 57, 43, 42, 47, 43, 48, 44, 40, 43, 42, 50, 42, 47, 38, 41, 38, 42, 44, 44, 52, 44, 44, 45, 43, 47, 46, 44, 46, 47, 48, 42, 46, 47, 35, 53, 51, 45, 47, 43, 43, 46, 43, 50, 43, 32, 48, 41, 48, 52, 49, 59, 52, 44, 49, 31, 46, 50, 45, 47, 46, 44, 40, 45, 47, 44, 47, 42, 42, 43, 40, 42, 47, 43, 40, 47, 40, 53, 48, 49, 42, 52, 45, 47, 39, 47, 49, 50, 44, 41, 44, 51, 46, 47, 43, 48, 45, 40, 48, 43, 46, 39, 40, 50, 46, 49, 41, 47, 39, 41, 43, 43, 47, 39, 52, 47, 35, 47, 47, 44, 35, 46, 42, 40, 45, 40, 46, 45, 46, 44, 40, 47, 47, 44, 50, 33, 50, 52, 45, 44, 53, 49, 50, 49, 44, 46, 47, 44, 43, 42, 43, 45, 41, 43, 43, 51, 45, 42, 40, 50, 48, 47, 34, 38, 45, 29, 40, 40, 46, 48, 46, 49, 43, 53, 48, 46, 49, 48, 53, 52, 44, 43, 39, 47, 47, 47, 53, 46, 45, 46, 46, 47, 47, 54, 42, 42, 42, 44, 45, 39, 49, 48, 34, 45, 36, 49, 37, 45, 45, 50, 44, 45, 54, 54, 54, 58, 45, 46, 47, 51, 41, 51, 51, 45, 51, 44, 52, 37, 42, 46, 43, 51, 49, 41, 49, 47, 47, 41, 30, 51, 33, 45, 30, 54, 43, 50, 44, 47, 41, 43, 45, 43, 46, 54, 49, 51, 42, 44, 44, 48, 35, 49, 49, 47, 43, 43, 48, 48, 47, 46, 51, 45, 44, 40, 40, 43, 48, 49, 37, 45, 51, 49, 45, 53, 46, 43, 51, 48, 48, 53, 44, 49, 40, 53, 43, 42, 48, 45, 39, 45, 45, 51, 38, 48, 48, 46, 46, 52, 46, 50, 51, 49, 50, 48, 52, 44, 51, 51, 48, 50, 51, 46, 40, 43, 40, 42, 43, 45, 45, 45, 43, 51, 37, 48, 45, 44, 43, 46, 44]\n",
      "TEST Variance 22.47183140476206\n",
      "\n",
      " LightGBM Model\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66741\n",
      "[LightGBM] [Info] Number of data points in the train set: 1280, number of used features: 310\n",
      "[LightGBM] [Info] Start training from score 43.636719\n",
      "\n",
      "VALID MAE:9.9625\n",
      "LABEL Variance 195.26875\n",
      "VALID Variance 60.553125\n",
      "[34, 45, 27, 47, 50, 34, 40, 44, 29, 47, 50, 42, 52, 41, 41, 45, 39, 50, 47, 47, 43, 46, 51, 48, 45, 49, 39, 38, 33, 48, 44, 47, 49, 47, 39, 45, 41, 42, 37, 36, 38, 49, 44, 49, 48, 45, 52, 49, 53, 47, 50, 51, 42, 48, 51, 49, 49, 50, 52, 47, 46, 43, 50, 36, 50, 44, 41, 47, 48, 42, 49, 44, 46, 51, 54, 48, 46, 44, 42, 44, 52, 45, 55, 53, 33, 51, 51, 49, 45, 51, 49, 55, 37, 46, 43, 24, 51, 50, 40, 55, 46, 50, 49, 48, 45, 34, 49, 40, 40, 48, 51, 51, 46, 51, 48, 51, 50, 49, 52, 52, 46, 41, 49, 41, 39, 44, 46, 44, 52, 49, 51, 54, 50, 49, 35, 43, 50, 54, 50, 47, 51, 51, 50, 53, 50, 52, 45, 48, 51, 55, 48, 49, 40, 44, 47, 44, 53, 48, 38, 40, 43, 44, 46, 42, 48, 45, 34, 41, 53, 40, 37, 39, 42, 39, 40, 44, 51, 49, 44, 42, 52, 38, 39, 46, 37, 30, 54, 54, 44, 51, 49, 50, 41, 46, 45, 44, 45, 45, 45, 47, 48, 49, 45, 49, 49, 47, 48, 42, 48, 47, 46, 39, 29, 36, 44, 24, 43, 40, 45, 46, 42, 48, 40, 52, 40, 48, 45, 49, 46, 44, 42, 46, 52, 47, 50, 51, 45, 45, 46, 46, 49, 49, 52, 47, 43, 44, 44, 46, 43, 35, 49, 44, 26, 42, 28, 55, 27, 42, 37, 50, 48, 52, 52, 50, 55, 55, 41, 49, 44, 54, 41, 61, 47, 50, 59, 41, 49, 33, 50, 46, 40, 47, 50, 41, 48, 50, 48, 34, 29, 51, 26, 46, 24, 50, 45, 50, 34, 43, 46, 39, 40, 43, 50, 50, 50, 56, 39, 40, 49, 50, 25, 38, 50, 52, 40, 47, 47, 43, 52, 46, 47, 49, 45, 42, 39, 47, 49, 39, 37, 43, 54, 55, 44, 52, 45, 40, 58, 51, 45, 51, 43, 43, 47, 50, 48, 41, 52, 49, 37, 46, 44, 44, 41, 48, 52, 46, 48, 56, 45, 49, 43, 48, 54, 53, 47, 52, 54, 51, 57, 48, 45, 50, 46, 45, 45, 45, 42, 46, 48, 45, 47, 57, 49, 51, 47, 51, 51, 45, 42]\n",
      "TEST Variance 37.212178084998115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 将数据集分为训练集和测试集\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_x, train_y, test_size=0.2, random_state=0)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
    "# y_valid = torch.tensor(y_valid, dtype=torch.float32)\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(test_x.shape)\n",
    "\n",
    "# RVR\n",
    "print('\\n RVR Model')\n",
    "\n",
    "rbf_model = RVR(kernel=\"rbf\")\n",
    "rbf_model.fit(X_train, y_train)\n",
    "rbf_model_valid_prediction = rbf_model.predict(X_valid)\n",
    "print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in rbf_model_valid_prediction])))\n",
    "print('VALID Variance {}'.format(np.var([int(i) for i in rbf_model_valid_prediction])))\n",
    "print('LABEL Variance {}'.format(np.var([int(i) for i in y_valid])))\n",
    "rbf_model_test_prediction = rbf_model.predict(test_x)\n",
    "print([int(i) for i in rbf_model_test_prediction])\n",
    "print('TEST Variance {}'.format(np.var([int(i) for i in rbf_model_test_prediction])))\n",
    "\n",
    "# XGBoost\n",
    "print('\\n XGBoost Model')\n",
    "\n",
    "xgb_model = XGBRegressor(n_estimators=300, learning_rate=0.1)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_model_valid_prediction = xgb_model.predict(X_valid)\n",
    "\n",
    "print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in xgb_model_valid_prediction])))\n",
    "print('LABEL Variance {}'.format(np.var([int(i) for i in y_valid])))\n",
    "print('VALID Variance {}'.format(np.var([int(i) for i in xgb_model_valid_prediction])))\n",
    "xgb_model_test_prediction = xgb_model.predict(test_x)\n",
    "print([int(i) for i in xgb_model_test_prediction])\n",
    "print('TEST Variance {}'.format(np.var([int(i) for i in xgb_model_test_prediction])))\n",
    "\n",
    "# LightGBM\n",
    "print('\\n LightGBM Model')\n",
    "\n",
    "lgbm_model = LGBMRegressor()\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "lgbm_model_valid_prediction = lgbm_model.predict(X_valid)\n",
    "print('\\nVALID MAE:{}'.format(mean_absolute_error(y_valid, [int(i) for i in lgbm_model_valid_prediction])))\n",
    "print('LABEL Variance {}'.format(np.var([int(i) for i in y_valid])))\n",
    "print('VALID Variance {}'.format(np.var([int(i) for i in lgbm_model_valid_prediction])))\n",
    "lgbm_model_test_prediction = lgbm_model.predict(test_x)\n",
    "print([int(i) for i in lgbm_model_test_prediction])\n",
    "print('TEST Variance {}'.format(np.var([int(i) for i in lgbm_model_test_prediction])))\n",
    "\n",
    "# 集成模型\n",
    "# print('\\n Ensemble Model')\n",
    "\n",
    "# new_linear_pred, new_rbf_pred, new_poly_pred = [int(i) for i in linear_model_valid_prediction], [int(i) for i in rbf_model_valid_prediction], [int(i) for i in poly_model_valid_prediction]\n",
    "# new_rf_pred = [int(i) for i in rf_model_valid_prediction]\n",
    "# new_bag_pred = [int(i) for i in bag_model_valid_prediction]\n",
    "# new_xgb_pred = [int(i) for i in xgb_model_valid_prediction]\n",
    "# new_lgbm_pred = [int(i) for i in lgbm_model_valid_prediction]\n",
    "\n",
    "# # ensemble_prediction = [(i + j) / 2 for i, j in zip(new_rf_pred, new_rbf_pred)]\n",
    "# ensemble_prediction = [(i + j + k) / 3 for i, j, k in zip(new_xgb_pred, new_bag_pred, new_linear_pred)]\n",
    "# print('\\nENSEMBLE VALID MAE:{}'.format(mean_absolute_error(y_valid, ensemble_prediction)))\n",
    "# print('ENSEMBLE VALID Variance {}'.format(np.var([int(i) for i in ensemble_prediction])))\n",
    "\n",
    "# test_new_linear_pred, test_new_rbf_pred, test_new_poly_pred = [int(i) for i in linear_model_test_prediction], [\n",
    "#     int(i) for i in rbf_model_test_prediction], [int(i) for i in poly_model_test_prediction]\n",
    "# test_new_bag_pred = [int(i) for i in bag_model_test_prediction]\n",
    "# test_new_rf_pred = [int(i) for i in rf_model_test_prediction]\n",
    "# test_new_xgb_pref = [int(i) for i in xgb_model_test_prediction]\n",
    "\n",
    "# # test_ensemble_prediction = [(i + j) / 2 for i, j in zip(test_new_rf_pred, test_new_rbf_pred)]\n",
    "# test_ensemble_prediction = [(i + j + k) / 3 for i, j, k in\n",
    "#                             zip(test_new_xgb_pref, test_new_bag_pred, test_new_linear_pred)]\n",
    "# print([int(i) for i in test_ensemble_prediction])\n",
    "# print('TEST Variance {}'.format(np.var([int(i) for i in test_ensemble_prediction])))\n",
    "\n",
    "# 写结果\n",
    "# final_list = [int(i) for i in [int(i) for i in lgbm_model_test_prediction]]\n",
    "# submission_version_name = 'Ensemble-DataAug3-XGBoost_Bagging_RVR'\n",
    "# write_submission(final_list, submission_version_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
